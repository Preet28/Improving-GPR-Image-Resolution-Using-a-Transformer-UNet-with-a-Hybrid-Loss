{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6e1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Train: 6300, Val: 1350, Test: 1350\n",
      "Epoch [1/50] Train Loss: 0.012190, Val Loss: 0.000855\n",
      "  ✅ Saved Best Model at Epoch 1\n",
      "Epoch [2/50] Train Loss: 0.000746, Val Loss: 0.000649\n",
      "  ✅ Saved Best Model at Epoch 2\n",
      "Epoch [3/50] Train Loss: 0.000632, Val Loss: 0.000582\n",
      "  ✅ Saved Best Model at Epoch 3\n",
      "Epoch [4/50] Train Loss: 0.000566, Val Loss: 0.000547\n",
      "  ✅ Saved Best Model at Epoch 4\n",
      "Epoch [5/50] Train Loss: 0.000522, Val Loss: 0.000488\n",
      "  ✅ Saved Best Model at Epoch 5\n",
      "Epoch [6/50] Train Loss: 0.000483, Val Loss: 0.000465\n",
      "  ✅ Saved Best Model at Epoch 6\n",
      "Epoch [7/50] Train Loss: 0.000460, Val Loss: 0.000457\n",
      "  ✅ Saved Best Model at Epoch 7\n",
      "Epoch [8/50] Train Loss: 0.000440, Val Loss: 0.000422\n",
      "  ✅ Saved Best Model at Epoch 8\n",
      "Epoch [9/50] Train Loss: 0.000425, Val Loss: 0.000446\n",
      "Epoch [10/50] Train Loss: 0.000414, Val Loss: 0.000397\n",
      "  ✅ Saved Best Model at Epoch 10\n",
      "Epoch [11/50] Train Loss: 0.000398, Val Loss: 0.000395\n",
      "  ✅ Saved Best Model at Epoch 11\n",
      "Epoch [12/50] Train Loss: 0.000391, Val Loss: 0.000391\n",
      "  ✅ Saved Best Model at Epoch 12\n",
      "Epoch [13/50] Train Loss: 0.000383, Val Loss: 0.000374\n",
      "  ✅ Saved Best Model at Epoch 13\n",
      "Epoch [14/50] Train Loss: 0.000370, Val Loss: 0.000371\n",
      "  ✅ Saved Best Model at Epoch 14\n",
      "Epoch [15/50] Train Loss: 0.000370, Val Loss: 0.000359\n",
      "  ✅ Saved Best Model at Epoch 15\n",
      "Epoch [16/50] Train Loss: 0.000359, Val Loss: 0.000353\n",
      "  ✅ Saved Best Model at Epoch 16\n",
      "Epoch [17/50] Train Loss: 0.000357, Val Loss: 0.000350\n",
      "  ✅ Saved Best Model at Epoch 17\n",
      "Epoch [18/50] Train Loss: 0.000350, Val Loss: 0.000346\n",
      "  ✅ Saved Best Model at Epoch 18\n",
      "Epoch [19/50] Train Loss: 0.000346, Val Loss: 0.000361\n",
      "Epoch [20/50] Train Loss: 0.000338, Val Loss: 0.000376\n",
      "Epoch [21/50] Train Loss: 0.000333, Val Loss: 0.000348\n",
      "Epoch [22/50] Train Loss: 0.000330, Val Loss: 0.000330\n",
      "  ✅ Saved Best Model at Epoch 22\n",
      "Epoch [23/50] Train Loss: 0.000323, Val Loss: 0.000340\n",
      "Epoch [24/50] Train Loss: 0.000323, Val Loss: 0.000326\n",
      "  ✅ Saved Best Model at Epoch 24\n",
      "Epoch [25/50] Train Loss: 0.000314, Val Loss: 0.000323\n",
      "  ✅ Saved Best Model at Epoch 25\n",
      "Epoch [26/50] Train Loss: 0.000312, Val Loss: 0.000332\n",
      "Epoch [27/50] Train Loss: 0.000306, Val Loss: 0.000332\n",
      "Epoch [28/50] Train Loss: 0.000306, Val Loss: 0.000319\n",
      "  ✅ Saved Best Model at Epoch 28\n",
      "Epoch [29/50] Train Loss: 0.000302, Val Loss: 0.000319\n",
      "Epoch [30/50] Train Loss: 0.000296, Val Loss: 0.000314\n",
      "  ✅ Saved Best Model at Epoch 30\n",
      "Epoch [31/50] Train Loss: 0.000293, Val Loss: 0.000325\n",
      "Epoch [32/50] Train Loss: 0.000288, Val Loss: 0.000325\n",
      "Epoch [33/50] Train Loss: 0.000291, Val Loss: 0.000319\n",
      "Epoch [34/50] Train Loss: 0.000283, Val Loss: 0.000303\n",
      "  ✅ Saved Best Model at Epoch 34\n",
      "Epoch [35/50] Train Loss: 0.000282, Val Loss: 0.000311\n",
      "Epoch [36/50] Train Loss: 0.000278, Val Loss: 0.000305\n",
      "Epoch [37/50] Train Loss: 0.000277, Val Loss: 0.000306\n",
      "Epoch [38/50] Train Loss: 0.000272, Val Loss: 0.000300\n",
      "  ✅ Saved Best Model at Epoch 38\n",
      "Epoch [39/50] Train Loss: 0.000269, Val Loss: 0.000299\n",
      "  ✅ Saved Best Model at Epoch 39\n",
      "Epoch [40/50] Train Loss: 0.000271, Val Loss: 0.000305\n",
      "Epoch [41/50] Train Loss: 0.000268, Val Loss: 0.000299\n",
      "Epoch [42/50] Train Loss: 0.000262, Val Loss: 0.000294\n",
      "  ✅ Saved Best Model at Epoch 42\n",
      "Epoch [43/50] Train Loss: 0.000261, Val Loss: 0.000293\n",
      "  ✅ Saved Best Model at Epoch 43\n",
      "Epoch [44/50] Train Loss: 0.000259, Val Loss: 0.000292\n",
      "  ✅ Saved Best Model at Epoch 44\n",
      "Epoch [45/50] Train Loss: 0.000257, Val Loss: 0.000294\n",
      "Epoch [46/50] Train Loss: 0.000255, Val Loss: 0.000292\n",
      "  ✅ Saved Best Model at Epoch 46\n",
      "Epoch [47/50] Train Loss: 0.000253, Val Loss: 0.000289\n",
      "  ✅ Saved Best Model at Epoch 47\n",
      "Epoch [48/50] Train Loss: 0.000253, Val Loss: 0.000286\n",
      "  ✅ Saved Best Model at Epoch 48\n",
      "Epoch [49/50] Train Loss: 0.000249, Val Loss: 0.000288\n",
      "Epoch [50/50] Train Loss: 0.000247, Val Loss: 0.000284\n",
      "  ✅ Saved Best Model at Epoch 50\n",
      "\n",
      "Running inference on test set...\n",
      "Predictions saved in C:\\Preet\\9000_paired_bscans\\predictions_transunet_huber+ssim\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "DATASET_DIR = r\"C:\\Preet\\9000_paired_bscans\"  # *_l.png and *_h.png paired images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH = os.path.join(DATASET_DIR, \"best_transunet_huber+ssim.pth\")\n",
    "RESULTS_DIR = os.path.join(DATASET_DIR, \"predictions_transunet_huber+ssim\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, x_paths, y_paths):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(Image.open(self.x_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        y = np.array(Image.open(self.y_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        x = torch.tensor(x).unsqueeze(0)  # (1,H,W)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    low_paths, high_paths = [], []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\"_l.png\"):\n",
    "            low_path = os.path.join(dataset_dir, file)\n",
    "            high_path = os.path.join(dataset_dir, file.replace(\"_l.png\", \"_h.png\"))\n",
    "            if os.path.exists(high_path):\n",
    "                low_paths.append(low_path)\n",
    "                high_paths.append(high_path)\n",
    "    return low_paths, high_paths\n",
    "\n",
    "all_x, all_y = load_data(DATASET_DIR)\n",
    "\n",
    "# Split 70/15/15\n",
    "train_x, temp_x, train_y, temp_y = train_test_split(all_x, all_y, test_size=0.30, random_state=42)\n",
    "val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_x)}, Val: {len(val_x)}, Test: {len(test_x)}\")\n",
    "\n",
    "train_loader = DataLoader(GPRDataset(train_x, train_y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(GPRDataset(val_x, val_y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(GPRDataset(test_x, test_y), batch_size=1, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# class AttentionGate(nn.Module):\n",
    "#     def __init__(self, g_ch, x_ch, inter_ch):\n",
    "#         super().__init__()\n",
    "#         self.W_g = nn.Conv2d(g_ch, inter_ch, 1)\n",
    "#         self.W_x = nn.Conv2d(x_ch, inter_ch, 1)\n",
    "#         self.psi = nn.Conv2d(inter_ch, 1, 1)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, g, x):\n",
    "#         g1 = self.W_g(g)\n",
    "#         x1 = self.W_x(x)\n",
    "#         psi = self.relu(g1 + x1)\n",
    "#         psi = self.sigmoid(self.psi(psi))\n",
    "#         return x * psi\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None  # initialized at runtime\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)  # (B, N, patch_dim)\n",
    "        patches = self.project(patches)\n",
    "\n",
    "        patches = self.transformer(patches)\n",
    "\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # ================= Encoder =================\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)          # 1 -> 64\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)     # 64 -> 128\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)   # 128 -> 256\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ================= Bottleneck =================\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "\n",
    "        # ================= Decoder =================\n",
    "        # dec3 input: bottleneck + skip e3 = 256 + 256 = 512 channels\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)  # 512 -> 128\n",
    "        # dec2 input: dec3 output + skip e2 = 128 + 128 = 256 channels\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)    # 256 -> 64\n",
    "\n",
    "        # final conv\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)  # 64 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Encoder ----------------\n",
    "        e1 = self.enc1(x)                  # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))      # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))      # [B,256,H/4,W/4]\n",
    "\n",
    "        # ---------------- Bottleneck ----------------\n",
    "        b = self.bottleneck(self.pool(e3)) # [B,256,H/8,W/8]\n",
    "\n",
    "        # ---------------- Decoder ----------------\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))  # [B,128,H/4,W/4]\n",
    "\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))  # [B,64,H/2,W/2]\n",
    "\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        out = self.out_conv(d1)                        # [B,1,H,W]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "\n",
    "\n",
    "from pytorch_msssim import ssim  # pip install pytorch-msssim\n",
    "\n",
    "alpha = 0.8\n",
    "beta  = 0.2\n",
    "\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "\n",
    "        # Huber Loss\n",
    "        huber_loss = criterion(preds, y)\n",
    "\n",
    "        # SSIM Loss (1 - SSIM)\n",
    "        ssim_loss = 1 - ssim(preds, y, data_range=1.0, size_average=True)  # assumes preds/y in [0,1]\n",
    "\n",
    "        # Combined Loss\n",
    "        loss = alpha * huber_loss + beta * ssim_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "\n",
    "            huber_loss = criterion(preds, y)\n",
    "            ssim_loss = 1 - ssim(preds, y, data_range=1.0, size_average=True)\n",
    "            loss = alpha * huber_loss + beta * ssim_loss\n",
    "\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  ✅ Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE\n",
    "# ==========================\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).cpu().squeeze(0).squeeze(0).numpy()\n",
    "    pred_img = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    Image.fromarray(pred_img).save(os.path.join(RESULTS_DIR, f\"pred_{i+1}.png\"))\n",
    "\n",
    "print(f\"Predictions saved in {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96a8ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test Set Evaluation ----\n",
      "SSIM: avg=0.8929, min=0.7065, max=0.9982\n",
      "PSNR: avg=35.29 dB, min=28.64 dB, max=48.84 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "import shutil\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "RESULTS_DIR = r\"C:\\Preet\\9000_paired_bscans\\predictions_transunet_huber+ssim\"  # predictions from TransUNet\n",
    "GT_TEST_DIR = r\"C:\\Preet\\9000_paired_bscans\\ground_truth_test_transunet_huber+ssim\"\n",
    "\n",
    "# Copy ground truth test images to a dedicated folder\n",
    "os.makedirs(GT_TEST_DIR, exist_ok=True)\n",
    "for f in test_y:  # test_y comes from your train/val/test split\n",
    "    shutil.copy(f, GT_TEST_DIR)\n",
    "\n",
    "# ========================== FUNCTIONS ==========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "# ========================== MAIN EVALUATION ==========================\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "gt_files   = sorted([f for f in os.listdir(GT_TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "num_pairs = min(len(pred_files), len(gt_files))\n",
    "if num_pairs == 0:\n",
    "    print(\"[Error] No matching image files found in both directories.\")\n",
    "else:\n",
    "    if len(pred_files) != len(gt_files):\n",
    "        print(f\"[Warning] Different number of images: Predictions={len(pred_files)}, Ground Truth={len(gt_files)}\")\n",
    "        print(f\"Evaluating only first {num_pairs} matched pairs.\")\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        pred_path = os.path.join(RESULTS_DIR, pred_files[i])\n",
    "        gt_path   = os.path.join(GT_TEST_DIR, gt_files[i])\n",
    "\n",
    "        pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_img   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if pred_img is None or gt_img is None:\n",
    "            print(f\"[Error] Could not load: {pred_files[i]} or {gt_files[i]}\")\n",
    "            continue\n",
    "\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"\\n---- Test Set Evaluation ----\")\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad13b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating png_images_650M_1083M ----\n",
      "SSIM: avg=0.9559, min=0.9274, max=0.9672\n",
      "PSNR: avg=35.56 dB, min=32.54 dB, max=36.83 dB\n",
      "\n",
      "---- Evaluating png_images_700M_1167M ----\n",
      "SSIM: avg=0.9811, min=0.7581, max=0.9856\n",
      "PSNR: avg=36.55 dB, min=29.39 dB, max=37.53 dB\n",
      "\n",
      "---- Evaluating png_images_800M_1333M ----\n",
      "SSIM: avg=0.9906, min=0.9728, max=0.9941\n",
      "PSNR: avg=38.89 dB, min=34.09 dB, max=40.08 dB\n",
      "\n",
      "---- Evaluating png_images_850M_1416M ----\n",
      "SSIM: avg=0.9650, min=0.9342, max=0.9752\n",
      "PSNR: avg=34.06 dB, min=31.07 dB, max=35.52 dB\n",
      "\n",
      "---- Evaluating png_images_900M_1500M ----\n",
      "SSIM: avg=0.9344, min=0.8776, max=0.9532\n",
      "PSNR: avg=33.95 dB, min=30.09 dB, max=35.40 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FREQ_DIRS = [\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_650M_1083M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_700M_1167M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_800M_1333M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_850M_1416M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_900M_1500M\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\9000_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE LOOP\n",
    "# =========================\n",
    "for freq_dir in FREQ_DIRS:\n",
    "    print(f\"\\n---- Evaluating {os.path.basename(freq_dir)} ----\")\n",
    "    pred_dir = os.path.join(freq_dir, \"predictions_transunet_huber+ssim\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    low_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_l.png\")])\n",
    "    high_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_h.png\")])\n",
    "\n",
    "    psnr_values, ssim_values = [], []\n",
    "\n",
    "    for i in range(len(low_files)):\n",
    "        low_path = os.path.join(freq_dir, low_files[i])\n",
    "        high_path = os.path.join(freq_dir, high_files[i])\n",
    "\n",
    "        # Load LR\n",
    "        lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32)/255.0\n",
    "        lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = model(lr_tensor).cpu().numpy()\n",
    "\n",
    "        # remove batch/channel dims and scale to 0-255\n",
    "        pred_img = np.squeeze(pred)\n",
    "        pred_img = (pred_img*255.0).clip(0,255).astype(np.uint8)\n",
    "        pred_img = cv2.normalize(pred_img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # save prediction\n",
    "        pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "        Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "        # Load GT\n",
    "        gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        # Metrics\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "    else:\n",
    "        print(\"[Error] No valid pairs processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6bf939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 650 → pred → compare with 1083 ----\n",
      "SSIM: avg=0.9541, min=0.9275, max=0.9674\n",
      "PSNR: avg=35.57 dB, min=32.63 dB, max=36.81 dB\n",
      "\n",
      "---- Stage 2: pred_1083 → pred → compare with 1800 ----\n",
      "SSIM: avg=0.9268, min=0.8927, max=0.9463\n",
      "PSNR: avg=30.13 dB, min=28.69 dB, max=30.88 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset\\png_images_650M_1083M_1800M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\9000_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    return cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"650_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1083_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1800_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_transunet_huber+ssim\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_transunet_huber+ssim\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32)/255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 650 → pred → compare with 1083 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1083 → pred → compare with 1800 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3870606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 900 → pred → compare with 1500 ----\n",
      "SSIM: avg=0.7192, min=0.5828, max=0.8514\n",
      "PSNR: avg=30.30 dB, min=29.25 dB, max=31.04 dB\n",
      "\n",
      "---- Stage 2: pred_1500 → pred → compare with 2500 ----\n",
      "SSIM: avg=0.7269, min=0.6029, max=0.8504\n",
      "PSNR: avg=27.91 dB, min=27.40 dB, max=28.67 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset\\png_images_900M_1500M_2500M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\9000_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    return cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"900_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1500_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"2500_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_transunet_huber+ssim\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_transunet_huber+ssim\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32)/255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 900 → pred → compare with 1500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1500 → pred → compare with 2500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947bbee",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18f241",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e80717",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b53b2",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3081037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Train: 2800, Val: 600, Test: 600\n",
      "Epoch [1/75] Train Loss: 0.031924, Val Loss: 0.001859\n",
      "  ✅ Saved Best Model at Epoch 1\n",
      "Epoch [2/75] Train Loss: 0.001310, Val Loss: 0.001274\n",
      "  ✅ Saved Best Model at Epoch 2\n",
      "Epoch [3/75] Train Loss: 0.000900, Val Loss: 0.000790\n",
      "  ✅ Saved Best Model at Epoch 3\n",
      "Epoch [4/75] Train Loss: 0.000745, Val Loss: 0.000791\n",
      "Epoch [5/75] Train Loss: 0.000688, Val Loss: 0.000721\n",
      "  ✅ Saved Best Model at Epoch 5\n",
      "Epoch [6/75] Train Loss: 0.000633, Val Loss: 0.000631\n",
      "  ✅ Saved Best Model at Epoch 6\n",
      "Epoch [7/75] Train Loss: 0.000595, Val Loss: 0.000581\n",
      "  ✅ Saved Best Model at Epoch 7\n",
      "Epoch [8/75] Train Loss: 0.000572, Val Loss: 0.000549\n",
      "  ✅ Saved Best Model at Epoch 8\n",
      "Epoch [9/75] Train Loss: 0.000540, Val Loss: 0.000657\n",
      "Epoch [10/75] Train Loss: 0.000545, Val Loss: 0.000525\n",
      "  ✅ Saved Best Model at Epoch 10\n",
      "Epoch [11/75] Train Loss: 0.000497, Val Loss: 0.000532\n",
      "Epoch [12/75] Train Loss: 0.000501, Val Loss: 0.000515\n",
      "  ✅ Saved Best Model at Epoch 12\n",
      "Epoch [13/75] Train Loss: 0.000478, Val Loss: 0.000470\n",
      "  ✅ Saved Best Model at Epoch 13\n",
      "Epoch [14/75] Train Loss: 0.000458, Val Loss: 0.000465\n",
      "  ✅ Saved Best Model at Epoch 14\n",
      "Epoch [15/75] Train Loss: 0.000466, Val Loss: 0.000461\n",
      "  ✅ Saved Best Model at Epoch 15\n",
      "Epoch [16/75] Train Loss: 0.000448, Val Loss: 0.000459\n",
      "  ✅ Saved Best Model at Epoch 16\n",
      "Epoch [17/75] Train Loss: 0.000427, Val Loss: 0.000465\n",
      "Epoch [18/75] Train Loss: 0.000428, Val Loss: 0.000465\n",
      "Epoch [19/75] Train Loss: 0.000414, Val Loss: 0.000443\n",
      "  ✅ Saved Best Model at Epoch 19\n",
      "Epoch [20/75] Train Loss: 0.000411, Val Loss: 0.000472\n",
      "Epoch [21/75] Train Loss: 0.000409, Val Loss: 0.000424\n",
      "  ✅ Saved Best Model at Epoch 21\n",
      "Epoch [22/75] Train Loss: 0.000402, Val Loss: 0.000431\n",
      "Epoch [23/75] Train Loss: 0.000390, Val Loss: 0.000448\n",
      "Epoch [24/75] Train Loss: 0.000390, Val Loss: 0.000424\n",
      "  ✅ Saved Best Model at Epoch 24\n",
      "Epoch [25/75] Train Loss: 0.000377, Val Loss: 0.000404\n",
      "  ✅ Saved Best Model at Epoch 25\n",
      "Epoch [26/75] Train Loss: 0.000381, Val Loss: 0.000395\n",
      "  ✅ Saved Best Model at Epoch 26\n",
      "Epoch [27/75] Train Loss: 0.000371, Val Loss: 0.000431\n",
      "Epoch [28/75] Train Loss: 0.000375, Val Loss: 0.000403\n",
      "Epoch [29/75] Train Loss: 0.000365, Val Loss: 0.000392\n",
      "  ✅ Saved Best Model at Epoch 29\n",
      "Epoch [30/75] Train Loss: 0.000360, Val Loss: 0.000402\n",
      "Epoch [31/75] Train Loss: 0.000357, Val Loss: 0.000403\n",
      "Epoch [32/75] Train Loss: 0.000353, Val Loss: 0.000390\n",
      "  ✅ Saved Best Model at Epoch 32\n",
      "Epoch [33/75] Train Loss: 0.000350, Val Loss: 0.000454\n",
      "Epoch [34/75] Train Loss: 0.000358, Val Loss: 0.000392\n",
      "Epoch [35/75] Train Loss: 0.000347, Val Loss: 0.000390\n",
      "  ✅ Saved Best Model at Epoch 35\n",
      "Epoch [36/75] Train Loss: 0.000341, Val Loss: 0.000380\n",
      "  ✅ Saved Best Model at Epoch 36\n",
      "Epoch [37/75] Train Loss: 0.000337, Val Loss: 0.000387\n",
      "Epoch [38/75] Train Loss: 0.000341, Val Loss: 0.000377\n",
      "  ✅ Saved Best Model at Epoch 38\n",
      "Epoch [39/75] Train Loss: 0.000328, Val Loss: 0.000383\n",
      "Epoch [40/75] Train Loss: 0.000332, Val Loss: 0.000373\n",
      "  ✅ Saved Best Model at Epoch 40\n",
      "Epoch [41/75] Train Loss: 0.000327, Val Loss: 0.000389\n",
      "Epoch [42/75] Train Loss: 0.000326, Val Loss: 0.000367\n",
      "  ✅ Saved Best Model at Epoch 42\n",
      "Epoch [43/75] Train Loss: 0.000322, Val Loss: 0.000386\n",
      "Epoch [44/75] Train Loss: 0.000318, Val Loss: 0.000381\n",
      "Epoch [45/75] Train Loss: 0.000315, Val Loss: 0.000368\n",
      "Epoch [46/75] Train Loss: 0.000314, Val Loss: 0.000416\n",
      "Epoch [47/75] Train Loss: 0.000307, Val Loss: 0.000376\n",
      "Epoch [48/75] Train Loss: 0.000315, Val Loss: 0.000422\n",
      "Epoch [49/75] Train Loss: 0.000304, Val Loss: 0.000369\n",
      "Epoch [50/75] Train Loss: 0.000305, Val Loss: 0.000371\n",
      "Epoch [51/75] Train Loss: 0.000303, Val Loss: 0.000378\n",
      "Epoch [52/75] Train Loss: 0.000299, Val Loss: 0.000365\n",
      "  ✅ Saved Best Model at Epoch 52\n",
      "Epoch [53/75] Train Loss: 0.000297, Val Loss: 0.000377\n",
      "Epoch [54/75] Train Loss: 0.000291, Val Loss: 0.000359\n",
      "  ✅ Saved Best Model at Epoch 54\n",
      "Epoch [55/75] Train Loss: 0.000291, Val Loss: 0.000377\n",
      "Epoch [56/75] Train Loss: 0.000293, Val Loss: 0.000361\n",
      "Epoch [57/75] Train Loss: 0.000290, Val Loss: 0.000366\n",
      "Epoch [58/75] Train Loss: 0.000291, Val Loss: 0.000358\n",
      "  ✅ Saved Best Model at Epoch 58\n",
      "Epoch [59/75] Train Loss: 0.000279, Val Loss: 0.000364\n",
      "Epoch [60/75] Train Loss: 0.000284, Val Loss: 0.000361\n",
      "Epoch [61/75] Train Loss: 0.000280, Val Loss: 0.000361\n",
      "Epoch [62/75] Train Loss: 0.000282, Val Loss: 0.000398\n",
      "Epoch [63/75] Train Loss: 0.000275, Val Loss: 0.000354\n",
      "  ✅ Saved Best Model at Epoch 63\n",
      "Epoch [64/75] Train Loss: 0.000274, Val Loss: 0.000362\n",
      "Epoch [65/75] Train Loss: 0.000273, Val Loss: 0.000356\n",
      "Epoch [66/75] Train Loss: 0.000271, Val Loss: 0.000376\n",
      "Epoch [67/75] Train Loss: 0.000273, Val Loss: 0.000348\n",
      "  ✅ Saved Best Model at Epoch 67\n",
      "Epoch [68/75] Train Loss: 0.000266, Val Loss: 0.000355\n",
      "Epoch [69/75] Train Loss: 0.000267, Val Loss: 0.000353\n",
      "Epoch [70/75] Train Loss: 0.000266, Val Loss: 0.000361\n",
      "Epoch [71/75] Train Loss: 0.000268, Val Loss: 0.000359\n",
      "Epoch [72/75] Train Loss: 0.000263, Val Loss: 0.000341\n",
      "  ✅ Saved Best Model at Epoch 72\n",
      "Epoch [73/75] Train Loss: 0.000262, Val Loss: 0.000354\n",
      "Epoch [74/75] Train Loss: 0.000264, Val Loss: 0.000348\n",
      "Epoch [75/75] Train Loss: 0.000258, Val Loss: 0.000345\n",
      "\n",
      "Running inference on test set...\n",
      "Predictions saved in C:\\Preet\\4000_paired_bscans\\predictions_transunet_huber+ssim\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "DATASET_DIR = r\"C:\\Preet\\4000_paired_bscans\"  # *_l.png and *_h.png paired images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 75\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH = os.path.join(DATASET_DIR, \"best_transunet_huber+ssim.pth\")\n",
    "RESULTS_DIR = os.path.join(DATASET_DIR, \"predictions_transunet_huber+ssim\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, x_paths, y_paths):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(Image.open(self.x_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        y = np.array(Image.open(self.y_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        x = torch.tensor(x).unsqueeze(0)  # (1,H,W)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    low_paths, high_paths = [], []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\"_l.png\"):\n",
    "            low_path = os.path.join(dataset_dir, file)\n",
    "            high_path = os.path.join(dataset_dir, file.replace(\"_l.png\", \"_h.png\"))\n",
    "            if os.path.exists(high_path):\n",
    "                low_paths.append(low_path)\n",
    "                high_paths.append(high_path)\n",
    "    return low_paths, high_paths\n",
    "\n",
    "all_x, all_y = load_data(DATASET_DIR)\n",
    "\n",
    "# Split 70/15/15\n",
    "train_x, temp_x, train_y, temp_y = train_test_split(all_x, all_y, test_size=0.30, random_state=42)\n",
    "val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_x)}, Val: {len(val_x)}, Test: {len(test_x)}\")\n",
    "\n",
    "train_loader = DataLoader(GPRDataset(train_x, train_y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(GPRDataset(val_x, val_y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(GPRDataset(test_x, test_y), batch_size=1, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# class AttentionGate(nn.Module):\n",
    "#     def __init__(self, g_ch, x_ch, inter_ch):\n",
    "#         super().__init__()\n",
    "#         self.W_g = nn.Conv2d(g_ch, inter_ch, 1)\n",
    "#         self.W_x = nn.Conv2d(x_ch, inter_ch, 1)\n",
    "#         self.psi = nn.Conv2d(inter_ch, 1, 1)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, g, x):\n",
    "#         g1 = self.W_g(g)\n",
    "#         x1 = self.W_x(x)\n",
    "#         psi = self.relu(g1 + x1)\n",
    "#         psi = self.sigmoid(self.psi(psi))\n",
    "#         return x * psi\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None  # initialized at runtime\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)  # (B, N, patch_dim)\n",
    "        patches = self.project(patches)\n",
    "\n",
    "        patches = self.transformer(patches)\n",
    "\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # ================= Encoder =================\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)          # 1 -> 64\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)     # 64 -> 128\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)   # 128 -> 256\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ================= Bottleneck =================\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "\n",
    "        # ================= Decoder =================\n",
    "        # dec3 input: bottleneck + skip e3 = 256 + 256 = 512 channels\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)  # 512 -> 128\n",
    "        # dec2 input: dec3 output + skip e2 = 128 + 128 = 256 channels\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)    # 256 -> 64\n",
    "\n",
    "        # final conv\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)  # 64 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Encoder ----------------\n",
    "        e1 = self.enc1(x)                  # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))      # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))      # [B,256,H/4,W/4]\n",
    "\n",
    "        # ---------------- Bottleneck ----------------\n",
    "        b = self.bottleneck(self.pool(e3)) # [B,256,H/8,W/8]\n",
    "\n",
    "        # ---------------- Decoder ----------------\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))  # [B,128,H/4,W/4]\n",
    "\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))  # [B,64,H/2,W/2]\n",
    "\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        out = self.out_conv(d1)                        # [B,1,H,W]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "\n",
    "\n",
    "from pytorch_msssim import ssim  # pip install pytorch-msssim\n",
    "\n",
    "alpha = 0.8\n",
    "beta  = 0.2\n",
    "\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "\n",
    "        # Huber Loss\n",
    "        huber_loss = criterion(preds, y)\n",
    "\n",
    "        # SSIM Loss (1 - SSIM)\n",
    "        ssim_loss = 1 - ssim(preds, y, data_range=1.0, size_average=True)  # assumes preds/y in [0,1]\n",
    "\n",
    "        # Combined Loss\n",
    "        loss = alpha * huber_loss + beta * ssim_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "\n",
    "            huber_loss = criterion(preds, y)\n",
    "            ssim_loss = 1 - ssim(preds, y, data_range=1.0, size_average=True)\n",
    "            loss = alpha * huber_loss + beta * ssim_loss\n",
    "\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  ✅ Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE\n",
    "# ==========================\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).cpu().squeeze(0).squeeze(0).numpy()\n",
    "    pred_img = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    Image.fromarray(pred_img).save(os.path.join(RESULTS_DIR, f\"pred_{i+1}.png\"))\n",
    "\n",
    "print(f\"Predictions saved in {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ccd998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test Set Evaluation ----\n",
      "SSIM: avg=0.8915, min=0.7125, max=0.9974\n",
      "PSNR: avg=35.40 dB, min=28.93 dB, max=50.90 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "import shutil\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "RESULTS_DIR = r\"C:\\Preet\\4000_paired_bscans\\predictions_transunet_huber+ssim\"  # predictions from TransUNet\n",
    "GT_TEST_DIR = r\"C:\\Preet\\4000_paired_bscans\\ground_truth_test_transunet_huber+ssim\"\n",
    "\n",
    "# Copy ground truth test images to a dedicated folder\n",
    "os.makedirs(GT_TEST_DIR, exist_ok=True)\n",
    "for f in test_y:  # test_y comes from your train/val/test split\n",
    "    shutil.copy(f, GT_TEST_DIR)\n",
    "\n",
    "# ========================== FUNCTIONS ==========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "# ========================== MAIN EVALUATION ==========================\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "gt_files   = sorted([f for f in os.listdir(GT_TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "num_pairs = min(len(pred_files), len(gt_files))\n",
    "if num_pairs == 0:\n",
    "    print(\"[Error] No matching image files found in both directories.\")\n",
    "else:\n",
    "    if len(pred_files) != len(gt_files):\n",
    "        print(f\"[Warning] Different number of images: Predictions={len(pred_files)}, Ground Truth={len(gt_files)}\")\n",
    "        print(f\"Evaluating only first {num_pairs} matched pairs.\")\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        pred_path = os.path.join(RESULTS_DIR, pred_files[i])\n",
    "        gt_path   = os.path.join(GT_TEST_DIR, gt_files[i])\n",
    "\n",
    "        pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_img   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if pred_img is None or gt_img is None:\n",
    "            print(f\"[Error] Could not load: {pred_files[i]} or {gt_files[i]}\")\n",
    "            continue\n",
    "\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"\\n---- Test Set Evaluation ----\")\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85db4856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating png_images_650M_1083M ----\n",
      "SSIM: avg=0.9572, min=0.9274, max=0.9685\n",
      "PSNR: avg=35.19 dB, min=32.11 dB, max=36.48 dB\n",
      "\n",
      "---- Evaluating png_images_700M_1167M ----\n",
      "SSIM: avg=0.9823, min=0.7541, max=0.9875\n",
      "PSNR: avg=36.57 dB, min=29.17 dB, max=38.28 dB\n",
      "\n",
      "---- Evaluating png_images_800M_1333M ----\n",
      "SSIM: avg=0.9893, min=0.9718, max=0.9930\n",
      "PSNR: avg=37.24 dB, min=33.45 dB, max=38.14 dB\n",
      "\n",
      "---- Evaluating png_images_850M_1416M ----\n",
      "SSIM: avg=0.9649, min=0.9314, max=0.9759\n",
      "PSNR: avg=33.40 dB, min=31.36 dB, max=35.17 dB\n",
      "\n",
      "---- Evaluating png_images_900M_1500M ----\n",
      "SSIM: avg=0.9378, min=0.8767, max=0.9562\n",
      "PSNR: avg=33.12 dB, min=30.56 dB, max=33.89 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FREQ_DIRS = [\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_650M_1083M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_700M_1167M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_800M_1333M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_850M_1416M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_900M_1500M\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\4000_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE LOOP\n",
    "# =========================\n",
    "for freq_dir in FREQ_DIRS:\n",
    "    print(f\"\\n---- Evaluating {os.path.basename(freq_dir)} ----\")\n",
    "    pred_dir = os.path.join(freq_dir, \"predictions_transunet_huber+ssim\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    low_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_l.png\")])\n",
    "    high_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_h.png\")])\n",
    "\n",
    "    psnr_values, ssim_values = [], []\n",
    "\n",
    "    for i in range(len(low_files)):\n",
    "        low_path = os.path.join(freq_dir, low_files[i])\n",
    "        high_path = os.path.join(freq_dir, high_files[i])\n",
    "\n",
    "        # Load LR\n",
    "        lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32)/255.0\n",
    "        lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = model(lr_tensor).cpu().numpy()\n",
    "\n",
    "        # remove batch/channel dims and scale to 0-255\n",
    "        pred_img = np.squeeze(pred)\n",
    "        pred_img = (pred_img*255.0).clip(0,255).astype(np.uint8)\n",
    "        pred_img = cv2.normalize(pred_img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # save prediction\n",
    "        pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "        Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "        # Load GT\n",
    "        gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        # Metrics\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "    else:\n",
    "        print(\"[Error] No valid pairs processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf83132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 650 → pred → compare with 1083 ----\n",
      "SSIM: avg=0.9552, min=0.9251, max=0.9680\n",
      "PSNR: avg=35.14 dB, min=31.82 dB, max=36.37 dB\n",
      "\n",
      "---- Stage 2: pred_1083 → pred → compare with 1800 ----\n",
      "SSIM: avg=0.9244, min=0.8801, max=0.9424\n",
      "PSNR: avg=28.53 dB, min=27.84 dB, max=31.16 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset_4000\\png_images_650M_1083M_1800M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\4000_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    return cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"650_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1083_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1800_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_transunet_huber+ssim\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_transunet_huber+ssim\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32)/255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 650 → pred → compare with 1083 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1083 → pred → compare with 1800 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d15248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 900 → pred → compare with 1500 ----\n",
      "SSIM: avg=0.7291, min=0.5953, max=0.8614\n",
      "PSNR: avg=29.81 dB, min=28.60 dB, max=30.28 dB\n",
      "\n",
      "---- Stage 2: pred_1500 → pred → compare with 2500 ----\n",
      "SSIM: avg=0.7513, min=0.6396, max=0.8559\n",
      "PSNR: avg=27.60 dB, min=27.30 dB, max=28.15 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset_4000\\png_images_900M_1500M_2500M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\4000_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    return cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"900_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1500_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"2500_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_transunet_huber+ssim\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_transunet_huber+ssim\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32)/255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 900 → pred → compare with 1500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1500 → pred → compare with 2500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04389cf8",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fea947",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67967eb4",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb1035",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d0112",
   "metadata": {},
   "source": [
    "clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52451a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Train: 5810, Val: 1245, Test: 1245\n",
      "Epoch [1/75] Train Loss: 0.009544, Val Loss: 0.000565\n",
      "  ✅ Saved Best Model at Epoch 1\n",
      "Epoch [2/75] Train Loss: 0.000520, Val Loss: 0.000546\n",
      "  ✅ Saved Best Model at Epoch 2\n",
      "Epoch [3/75] Train Loss: 0.000439, Val Loss: 0.000497\n",
      "  ✅ Saved Best Model at Epoch 3\n",
      "Epoch [4/75] Train Loss: 0.000388, Val Loss: 0.000361\n",
      "  ✅ Saved Best Model at Epoch 4\n",
      "Epoch [5/75] Train Loss: 0.000362, Val Loss: 0.000345\n",
      "  ✅ Saved Best Model at Epoch 5\n",
      "Epoch [6/75] Train Loss: 0.000344, Val Loss: 0.000358\n",
      "Epoch [7/75] Train Loss: 0.000331, Val Loss: 0.000314\n",
      "  ✅ Saved Best Model at Epoch 7\n",
      "Epoch [8/75] Train Loss: 0.000325, Val Loss: 0.000305\n",
      "  ✅ Saved Best Model at Epoch 8\n",
      "Epoch [9/75] Train Loss: 0.000315, Val Loss: 0.000301\n",
      "  ✅ Saved Best Model at Epoch 9\n",
      "Epoch [10/75] Train Loss: 0.000306, Val Loss: 0.000298\n",
      "  ✅ Saved Best Model at Epoch 10\n",
      "Epoch [11/75] Train Loss: 0.000304, Val Loss: 0.000294\n",
      "  ✅ Saved Best Model at Epoch 11\n",
      "Epoch [12/75] Train Loss: 0.000297, Val Loss: 0.000286\n",
      "  ✅ Saved Best Model at Epoch 12\n",
      "Epoch [13/75] Train Loss: 0.000291, Val Loss: 0.000279\n",
      "  ✅ Saved Best Model at Epoch 13\n",
      "Epoch [14/75] Train Loss: 0.000286, Val Loss: 0.000281\n",
      "Epoch [15/75] Train Loss: 0.000282, Val Loss: 0.000265\n",
      "  ✅ Saved Best Model at Epoch 15\n",
      "Epoch [16/75] Train Loss: 0.000282, Val Loss: 0.000262\n",
      "  ✅ Saved Best Model at Epoch 16\n",
      "Epoch [17/75] Train Loss: 0.000274, Val Loss: 0.000264\n",
      "Epoch [18/75] Train Loss: 0.000271, Val Loss: 0.000261\n",
      "  ✅ Saved Best Model at Epoch 18\n",
      "Epoch [19/75] Train Loss: 0.000281, Val Loss: 0.000283\n",
      "Epoch [20/75] Train Loss: 0.000265, Val Loss: 0.000258\n",
      "  ✅ Saved Best Model at Epoch 20\n",
      "Epoch [21/75] Train Loss: 0.000263, Val Loss: 0.000254\n",
      "  ✅ Saved Best Model at Epoch 21\n",
      "Epoch [22/75] Train Loss: 0.000260, Val Loss: 0.000257\n",
      "Epoch [23/75] Train Loss: 0.000257, Val Loss: 0.000343\n",
      "Epoch [24/75] Train Loss: 0.000253, Val Loss: 0.000245\n",
      "  ✅ Saved Best Model at Epoch 24\n",
      "Epoch [25/75] Train Loss: 0.000252, Val Loss: 0.000257\n",
      "Epoch [26/75] Train Loss: 0.000246, Val Loss: 0.000287\n",
      "Epoch [27/75] Train Loss: 0.000248, Val Loss: 0.000250\n",
      "Epoch [28/75] Train Loss: 0.000240, Val Loss: 0.000237\n",
      "  ✅ Saved Best Model at Epoch 28\n",
      "Epoch [29/75] Train Loss: 0.000241, Val Loss: 0.000235\n",
      "  ✅ Saved Best Model at Epoch 29\n",
      "Epoch [30/75] Train Loss: 0.000237, Val Loss: 0.000242\n",
      "Epoch [31/75] Train Loss: 0.000236, Val Loss: 0.000241\n",
      "Epoch [32/75] Train Loss: 0.000236, Val Loss: 0.000240\n",
      "Epoch [33/75] Train Loss: 0.000231, Val Loss: 0.000239\n",
      "Epoch [34/75] Train Loss: 0.000228, Val Loss: 0.000265\n",
      "Epoch [35/75] Train Loss: 0.000227, Val Loss: 0.000230\n",
      "  ✅ Saved Best Model at Epoch 35\n",
      "Epoch [36/75] Train Loss: 0.000227, Val Loss: 0.000239\n",
      "Epoch [37/75] Train Loss: 0.000225, Val Loss: 0.000229\n",
      "  ✅ Saved Best Model at Epoch 37\n",
      "Epoch [38/75] Train Loss: 0.000225, Val Loss: 0.000226\n",
      "  ✅ Saved Best Model at Epoch 38\n",
      "Epoch [39/75] Train Loss: 0.000220, Val Loss: 0.000233\n",
      "Epoch [40/75] Train Loss: 0.000221, Val Loss: 0.000244\n",
      "Epoch [41/75] Train Loss: 0.000220, Val Loss: 0.000226\n",
      "Epoch [42/75] Train Loss: 0.000217, Val Loss: 0.000226\n",
      "Epoch [43/75] Train Loss: 0.000218, Val Loss: 0.000226\n",
      "  ✅ Saved Best Model at Epoch 43\n",
      "Epoch [44/75] Train Loss: 0.000216, Val Loss: 0.000225\n",
      "  ✅ Saved Best Model at Epoch 44\n",
      "Epoch [45/75] Train Loss: 0.000215, Val Loss: 0.000225\n",
      "  ✅ Saved Best Model at Epoch 45\n",
      "Epoch [46/75] Train Loss: 0.000212, Val Loss: 0.000230\n",
      "Epoch [47/75] Train Loss: 0.000212, Val Loss: 0.000228\n",
      "Epoch [48/75] Train Loss: 0.000212, Val Loss: 0.000225\n",
      "Epoch [49/75] Train Loss: 0.000211, Val Loss: 0.000233\n",
      "Epoch [50/75] Train Loss: 0.000211, Val Loss: 0.000223\n",
      "  ✅ Saved Best Model at Epoch 50\n",
      "Epoch [51/75] Train Loss: 0.000209, Val Loss: 0.000224\n",
      "Epoch [52/75] Train Loss: 0.000209, Val Loss: 0.000233\n",
      "Epoch [53/75] Train Loss: 0.000207, Val Loss: 0.000221\n",
      "  ✅ Saved Best Model at Epoch 53\n",
      "Epoch [54/75] Train Loss: 0.000207, Val Loss: 0.000222\n",
      "Epoch [55/75] Train Loss: 0.000206, Val Loss: 0.000220\n",
      "  ✅ Saved Best Model at Epoch 55\n",
      "Epoch [56/75] Train Loss: 0.000206, Val Loss: 0.000220\n",
      "  ✅ Saved Best Model at Epoch 56\n",
      "Epoch [57/75] Train Loss: 0.000204, Val Loss: 0.000223\n",
      "Epoch [58/75] Train Loss: 0.000204, Val Loss: 0.000218\n",
      "  ✅ Saved Best Model at Epoch 58\n",
      "Epoch [59/75] Train Loss: 0.000203, Val Loss: 0.000224\n",
      "Epoch [60/75] Train Loss: 0.000203, Val Loss: 0.000220\n",
      "Epoch [61/75] Train Loss: 0.000203, Val Loss: 0.000220\n",
      "Epoch [62/75] Train Loss: 0.000201, Val Loss: 0.000219\n",
      "Epoch [63/75] Train Loss: 0.000201, Val Loss: 0.000219\n",
      "Epoch [64/75] Train Loss: 0.000200, Val Loss: 0.000220\n",
      "Epoch [65/75] Train Loss: 0.000200, Val Loss: 0.000220\n",
      "Epoch [66/75] Train Loss: 0.000199, Val Loss: 0.000218\n",
      "  ✅ Saved Best Model at Epoch 66\n",
      "Epoch [67/75] Train Loss: 0.000199, Val Loss: 0.000222\n",
      "Epoch [68/75] Train Loss: 0.000198, Val Loss: 0.000219\n",
      "Epoch [69/75] Train Loss: 0.000197, Val Loss: 0.000221\n",
      "Epoch [70/75] Train Loss: 0.000197, Val Loss: 0.000217\n",
      "  ✅ Saved Best Model at Epoch 70\n",
      "Epoch [71/75] Train Loss: 0.000196, Val Loss: 0.000309\n",
      "Epoch [72/75] Train Loss: 0.000196, Val Loss: 0.000227\n",
      "Epoch [73/75] Train Loss: 0.000195, Val Loss: 0.000231\n",
      "Epoch [74/75] Train Loss: 0.000195, Val Loss: 0.000218\n",
      "Epoch [75/75] Train Loss: 0.000195, Val Loss: 0.000234\n",
      "\n",
      "Running inference on test set...\n",
      "Predictions saved in C:\\Preet\\clean_paired_bscans\\predictions_transunet_huber+ssim\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "DATASET_DIR = r\"C:\\Preet\\clean_paired_bscans\"  # *_l.png and *_h.png paired images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 75\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH = os.path.join(DATASET_DIR, \"best_transunet_huber+ssim.pth\")\n",
    "RESULTS_DIR = os.path.join(DATASET_DIR, \"predictions_transunet_huber+ssim\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, x_paths, y_paths):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(Image.open(self.x_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        y = np.array(Image.open(self.y_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        x = torch.tensor(x).unsqueeze(0)  # (1,H,W)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    low_paths, high_paths = [], []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\"_l.png\"):\n",
    "            low_path = os.path.join(dataset_dir, file)\n",
    "            high_path = os.path.join(dataset_dir, file.replace(\"_l.png\", \"_h.png\"))\n",
    "            if os.path.exists(high_path):\n",
    "                low_paths.append(low_path)\n",
    "                high_paths.append(high_path)\n",
    "    return low_paths, high_paths\n",
    "\n",
    "all_x, all_y = load_data(DATASET_DIR)\n",
    "\n",
    "# Split 70/15/15\n",
    "train_x, temp_x, train_y, temp_y = train_test_split(all_x, all_y, test_size=0.30, random_state=42)\n",
    "val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_x)}, Val: {len(val_x)}, Test: {len(test_x)}\")\n",
    "\n",
    "train_loader = DataLoader(GPRDataset(train_x, train_y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(GPRDataset(val_x, val_y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(GPRDataset(test_x, test_y), batch_size=1, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# class AttentionGate(nn.Module):\n",
    "#     def __init__(self, g_ch, x_ch, inter_ch):\n",
    "#         super().__init__()\n",
    "#         self.W_g = nn.Conv2d(g_ch, inter_ch, 1)\n",
    "#         self.W_x = nn.Conv2d(x_ch, inter_ch, 1)\n",
    "#         self.psi = nn.Conv2d(inter_ch, 1, 1)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, g, x):\n",
    "#         g1 = self.W_g(g)\n",
    "#         x1 = self.W_x(x)\n",
    "#         psi = self.relu(g1 + x1)\n",
    "#         psi = self.sigmoid(self.psi(psi))\n",
    "#         return x * psi\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None  # initialized at runtime\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)  # (B, N, patch_dim)\n",
    "        patches = self.project(patches)\n",
    "\n",
    "        patches = self.transformer(patches)\n",
    "\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # ================= Encoder =================\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)          # 1 -> 64\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)     # 64 -> 128\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)   # 128 -> 256\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ================= Bottleneck =================\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "\n",
    "        # ================= Decoder =================\n",
    "        # dec3 input: bottleneck + skip e3 = 256 + 256 = 512 channels\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)  # 512 -> 128\n",
    "        # dec2 input: dec3 output + skip e2 = 128 + 128 = 256 channels\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)    # 256 -> 64\n",
    "\n",
    "        # final conv\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)  # 64 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Encoder ----------------\n",
    "        e1 = self.enc1(x)                  # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))      # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))      # [B,256,H/4,W/4]\n",
    "\n",
    "        # ---------------- Bottleneck ----------------\n",
    "        b = self.bottleneck(self.pool(e3)) # [B,256,H/8,W/8]\n",
    "\n",
    "        # ---------------- Decoder ----------------\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))  # [B,128,H/4,W/4]\n",
    "\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))  # [B,64,H/2,W/2]\n",
    "\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        out = self.out_conv(d1)                        # [B,1,H,W]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "\n",
    "\n",
    "from pytorch_msssim import ssim  # pip install pytorch-msssim\n",
    "\n",
    "alpha = 0.8\n",
    "beta  = 0.2\n",
    "\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "\n",
    "        # Huber Loss\n",
    "        huber_loss = criterion(preds, y)\n",
    "\n",
    "        # SSIM Loss (1 - SSIM)\n",
    "        ssim_loss = 1 - ssim(preds, y, data_range=1.0, size_average=True)  # assumes preds/y in [0,1]\n",
    "\n",
    "        # Combined Loss\n",
    "        loss = alpha * huber_loss + beta * ssim_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "\n",
    "            huber_loss = criterion(preds, y)\n",
    "            ssim_loss = 1 - ssim(preds, y, data_range=1.0, size_average=True)\n",
    "            loss = alpha * huber_loss + beta * ssim_loss\n",
    "\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  ✅ Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE\n",
    "# ==========================\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).cpu().squeeze(0).squeeze(0).numpy()\n",
    "    pred_img = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    Image.fromarray(pred_img).save(os.path.join(RESULTS_DIR, f\"pred_{i+1}.png\"))\n",
    "\n",
    "print(f\"Predictions saved in {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f68021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test Set Evaluation ----\n",
      "SSIM: avg=0.9044, min=0.7484, max=0.9979\n",
      "PSNR: avg=35.99 dB, min=29.61 dB, max=52.27 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "import shutil\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "RESULTS_DIR = r\"C:\\Preet\\clean_paired_bscans\\predictions_transunet_huber+ssim\"  # predictions from TransUNet\n",
    "GT_TEST_DIR = r\"C:\\Preet\\clean_paired_bscans\\ground_truth_test_transunet_huber+ssim\"\n",
    "\n",
    "# Copy ground truth test images to a dedicated folder\n",
    "os.makedirs(GT_TEST_DIR, exist_ok=True)\n",
    "for f in test_y:  # test_y comes from your train/val/test split\n",
    "    shutil.copy(f, GT_TEST_DIR)\n",
    "\n",
    "# ========================== FUNCTIONS ==========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "# ========================== MAIN EVALUATION ==========================\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "gt_files   = sorted([f for f in os.listdir(GT_TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "num_pairs = min(len(pred_files), len(gt_files))\n",
    "if num_pairs == 0:\n",
    "    print(\"[Error] No matching image files found in both directories.\")\n",
    "else:\n",
    "    if len(pred_files) != len(gt_files):\n",
    "        print(f\"[Warning] Different number of images: Predictions={len(pred_files)}, Ground Truth={len(gt_files)}\")\n",
    "        print(f\"Evaluating only first {num_pairs} matched pairs.\")\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        pred_path = os.path.join(RESULTS_DIR, pred_files[i])\n",
    "        gt_path   = os.path.join(GT_TEST_DIR, gt_files[i])\n",
    "\n",
    "        pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_img   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if pred_img is None or gt_img is None:\n",
    "            print(f\"[Error] Could not load: {pred_files[i]} or {gt_files[i]}\")\n",
    "            continue\n",
    "\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"\\n---- Test Set Evaluation ----\")\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fdc45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating png_images_650M_1083M ----\n",
      "SSIM: avg=0.9622, min=0.9309, max=0.9731\n",
      "PSNR: avg=35.91 dB, min=30.69 dB, max=37.21 dB\n",
      "\n",
      "---- Evaluating png_images_700M_1167M ----\n",
      "SSIM: avg=0.9871, min=0.8320, max=0.9909\n",
      "PSNR: avg=37.91 dB, min=32.60 dB, max=38.81 dB\n",
      "\n",
      "---- Evaluating png_images_800M_1333M ----\n",
      "SSIM: avg=0.9858, min=0.9795, max=0.9887\n",
      "PSNR: avg=36.98 dB, min=34.60 dB, max=38.07 dB\n",
      "\n",
      "---- Evaluating png_images_850M_1416M ----\n",
      "SSIM: avg=0.9570, min=0.9341, max=0.9656\n",
      "PSNR: avg=34.74 dB, min=31.40 dB, max=35.94 dB\n",
      "\n",
      "---- Evaluating png_images_900M_1500M ----\n",
      "SSIM: avg=0.9309, min=0.8809, max=0.9470\n",
      "PSNR: avg=34.48 dB, min=31.15 dB, max=35.51 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FREQ_DIRS = [\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_650M_1083M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_700M_1167M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_800M_1333M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_850M_1416M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_900M_1500M\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE LOOP\n",
    "# =========================\n",
    "for freq_dir in FREQ_DIRS:\n",
    "    print(f\"\\n---- Evaluating {os.path.basename(freq_dir)} ----\")\n",
    "    pred_dir = os.path.join(freq_dir, \"predictions_transunet_huber+ssim\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    low_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_l.png\")])\n",
    "    high_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_h.png\")])\n",
    "\n",
    "    psnr_values, ssim_values = [], []\n",
    "\n",
    "    for i in range(len(low_files)):\n",
    "        low_path = os.path.join(freq_dir, low_files[i])\n",
    "        high_path = os.path.join(freq_dir, high_files[i])\n",
    "\n",
    "        # Load LR\n",
    "        lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32)/255.0\n",
    "        lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = model(lr_tensor).cpu().numpy()\n",
    "\n",
    "        # remove batch/channel dims and scale to 0-255\n",
    "        pred_img = np.squeeze(pred)\n",
    "        pred_img = (pred_img*255.0).clip(0,255).astype(np.uint8)\n",
    "        pred_img = cv2.normalize(pred_img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # save prediction\n",
    "        pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "        Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "        # Load GT\n",
    "        gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        # Metrics\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "    else:\n",
    "        print(\"[Error] No valid pairs processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751e13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 650 → pred → compare with 1083 ----\n",
      "SSIM: avg=0.9609, min=0.9387, max=0.9726\n",
      "PSNR: avg=35.82 dB, min=33.45 dB, max=37.16 dB\n",
      "\n",
      "---- Stage 2: pred_1083 → pred → compare with 1800 ----\n",
      "SSIM: avg=0.9243, min=0.8989, max=0.9428\n",
      "PSNR: avg=31.00 dB, min=29.74 dB, max=32.30 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\clean_validation dataset\\png_images_650M_1083M_1800M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    return cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"650_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1083_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1800_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_transunet_huber+ssim\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_transunet_huber+ssim\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32)/255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 650 → pred → compare with 1083 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1083 → pred → compare with 1800 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0fe147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 900 → pred → compare with 1500 ----\n",
      "SSIM: avg=0.7474, min=0.5995, max=0.8695\n",
      "PSNR: avg=29.93 dB, min=28.32 dB, max=30.53 dB\n",
      "\n",
      "---- Stage 2: pred_1500 → pred → compare with 2500 ----\n",
      "SSIM: avg=0.7693, min=0.6452, max=0.8678\n",
      "PSNR: avg=28.72 dB, min=27.50 dB, max=29.59 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\clean_validation dataset\\png_images_900M_1500M_2500M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = (pred * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    return cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"900_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1500_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"2500_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_transunet_huber+ssim\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_transunet_huber+ssim\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32)/255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 900 → pred → compare with 1500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1500 → pred → compare with 2500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75a671",
   "metadata": {},
   "source": [
    "for testing 400Mhz dataset on trained 750Mhz model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "TEST_DIR = r\"C:\\Preet\\400_670_Dataset\"\n",
    "\n",
    "print(f\"\\n---- Evaluating {TEST_DIR} ----\")\n",
    "pred_dir = os.path.join(TEST_DIR, \"predictions_unet+RB+AG+hybrid\")\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "low_files = numerical_sort([f for f in os.listdir(TEST_DIR) if f.endswith(\"_l.png\")])\n",
    "high_files = numerical_sort([f for f in os.listdir(TEST_DIR) if f.endswith(\"_h.png\")])\n",
    "\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "for i in range(len(low_files)):\n",
    "    low_path = os.path.join(TEST_DIR, low_files[i])\n",
    "    high_path = os.path.join(TEST_DIR, high_files[i])\n",
    "\n",
    "    # Load LR\n",
    "    lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred = model(lr_tensor).cpu().squeeze().numpy()\n",
    "\n",
    "    # Scale to 0–255\n",
    "    pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "    Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "    # Load GT\n",
    "    gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if pred_img.shape != gt_img.shape:\n",
    "        pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "    # Metrics\n",
    "    psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "    ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "if psnr_values and ssim_values:\n",
    "    print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "    print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "else:\n",
    "    print(\"[Error] No valid *_l.png and *_h.png pairs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166f017",
   "metadata": {},
   "source": [
    "for testing 400Mhz on trained 750Mhz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e026f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating C:\\Preet\\400_670_Dataset ----\n",
      "SSIM: avg=0.7523, min=0.6270, max=0.8185\n",
      "PSNR: avg=29.69 dB, min=29.07 dB, max=30.24 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "TEST_DIR = r\"C:\\Preet\\400_670_Dataset\"\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\best_transunet_huber+ssim.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*2)\n",
    "        self.dec2 = ConvBlock(base_ch*2 + base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x); e2 = self.enc2(self.pool(e1)); e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = F.interpolate(b, size=e3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n---- Evaluating {TEST_DIR} ----\")\n",
    "pred_dir = os.path.join(TEST_DIR, \"predictions_transunet_huber\")\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "low_files = numerical_sort([f for f in os.listdir(TEST_DIR) if f.endswith(\"_l.png\")])\n",
    "high_files = numerical_sort([f for f in os.listdir(TEST_DIR) if f.endswith(\"_h.png\")])\n",
    "\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "for i in range(len(low_files)):\n",
    "    low_path = os.path.join(TEST_DIR, low_files[i])\n",
    "    high_path = os.path.join(TEST_DIR, high_files[i])\n",
    "\n",
    "    # Load LR\n",
    "    lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred = model(lr_tensor).cpu().squeeze().numpy()\n",
    "\n",
    "    # Scale to 0–255\n",
    "    pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "    Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "    # Load GT\n",
    "    gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if pred_img.shape != gt_img.shape:\n",
    "        pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "    # Metrics\n",
    "    psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "    ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "if psnr_values and ssim_values:\n",
    "    print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "    print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "else:\n",
    "    print(\"[Error] No valid *_l.png and *_h.png pairs found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
