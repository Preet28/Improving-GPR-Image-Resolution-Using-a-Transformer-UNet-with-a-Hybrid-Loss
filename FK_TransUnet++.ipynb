{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea70d87",
   "metadata": {},
   "source": [
    "FK-TransUNet++ (GPR-SR) — TransUNet with CUP decoder & f–k spectral consistency for GPR super-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80165072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Train: 6300, Val: 1350, Test: 1350\n",
      "Epoch [1/60] Train 0.003464 | Val 0.000149\n",
      "  ✅ Saved Best Model at Epoch 1\n",
      "Epoch [2/60] Train 0.000120 | Val 0.000109\n",
      "  ✅ Saved Best Model at Epoch 2\n",
      "Epoch [3/60] Train 0.000107 | Val 0.000101\n",
      "  ✅ Saved Best Model at Epoch 3\n",
      "Epoch [4/60] Train 0.000097 | Val 0.000090\n",
      "  ✅ Saved Best Model at Epoch 4\n",
      "Epoch [5/60] Train 0.000091 | Val 0.000107\n",
      "Epoch [6/60] Train 0.000087 | Val 0.000081\n",
      "  ✅ Saved Best Model at Epoch 6\n",
      "Epoch [7/60] Train 0.000083 | Val 0.000088\n",
      "Epoch [8/60] Train 0.000074 | Val 0.000072\n",
      "  ✅ Saved Best Model at Epoch 8\n",
      "Epoch [9/60] Train 0.000068 | Val 0.000060\n",
      "  ✅ Saved Best Model at Epoch 9\n",
      "Epoch [10/60] Train 0.000064 | Val 0.000058\n",
      "  ✅ Saved Best Model at Epoch 10\n",
      "Epoch [11/60] Train 0.000061 | Val 0.000059\n",
      "Epoch [12/60] Train 0.000060 | Val 0.000058\n",
      "  ✅ Saved Best Model at Epoch 12\n",
      "Epoch [13/60] Train 0.000058 | Val 0.000061\n",
      "Epoch [14/60] Train 0.000057 | Val 0.000054\n",
      "  ✅ Saved Best Model at Epoch 14\n",
      "Epoch [15/60] Train 0.000055 | Val 0.000062\n",
      "Epoch [16/60] Train 0.000055 | Val 0.000056\n",
      "Epoch [17/60] Train 0.000054 | Val 0.000053\n",
      "  ✅ Saved Best Model at Epoch 17\n",
      "Epoch [18/60] Train 0.000053 | Val 0.000051\n",
      "  ✅ Saved Best Model at Epoch 18\n",
      "Epoch [19/60] Train 0.000053 | Val 0.000058\n",
      "Epoch [20/60] Train 0.000053 | Val 0.000053\n",
      "Epoch [21/60] Train 0.000052 | Val 0.000052\n",
      "Epoch [22/60] Train 0.000051 | Val 0.000054\n",
      "Epoch [23/60] Train 0.000051 | Val 0.000052\n",
      "Epoch [24/60] Train 0.000051 | Val 0.000056\n",
      "Epoch [25/60] Train 0.000050 | Val 0.000050\n",
      "  ✅ Saved Best Model at Epoch 25\n",
      "Epoch [26/60] Train 0.000051 | Val 0.000051\n",
      "Epoch [27/60] Train 0.000049 | Val 0.000049\n",
      "  ✅ Saved Best Model at Epoch 27\n",
      "Epoch [28/60] Train 0.000049 | Val 0.000049\n",
      "  ✅ Saved Best Model at Epoch 28\n",
      "Epoch [29/60] Train 0.000049 | Val 0.000049\n",
      "Epoch [30/60] Train 0.000049 | Val 0.000065\n",
      "Epoch [31/60] Train 0.000049 | Val 0.000051\n",
      "Epoch [32/60] Train 0.000049 | Val 0.000049\n",
      "  ✅ Saved Best Model at Epoch 32\n",
      "Epoch [33/60] Train 0.000047 | Val 0.000048\n",
      "  ✅ Saved Best Model at Epoch 33\n",
      "Epoch [34/60] Train 0.000048 | Val 0.000055\n",
      "Epoch [35/60] Train 0.000048 | Val 0.000048\n",
      "  ✅ Saved Best Model at Epoch 35\n",
      "Epoch [36/60] Train 0.000048 | Val 0.000049\n",
      "Epoch [37/60] Train 0.000047 | Val 0.000048\n",
      "  ✅ Saved Best Model at Epoch 37\n",
      "Epoch [38/60] Train 0.000047 | Val 0.000061\n",
      "Epoch [39/60] Train 0.000047 | Val 0.000048\n",
      "Epoch [40/60] Train 0.000046 | Val 0.000047\n",
      "  ✅ Saved Best Model at Epoch 40\n",
      "Epoch [41/60] Train 0.000046 | Val 0.000052\n",
      "Epoch [42/60] Train 0.000046 | Val 0.000048\n",
      "Epoch [43/60] Train 0.000046 | Val 0.000047\n",
      "  ✅ Saved Best Model at Epoch 43\n",
      "Epoch [44/60] Train 0.000046 | Val 0.000048\n",
      "Epoch [45/60] Train 0.000045 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 45\n",
      "Epoch [46/60] Train 0.000045 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 46\n",
      "Epoch [47/60] Train 0.000045 | Val 0.000054\n",
      "Epoch [48/60] Train 0.000045 | Val 0.000047\n",
      "Epoch [49/60] Train 0.000045 | Val 0.000055\n",
      "Epoch [50/60] Train 0.000045 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 50\n",
      "Epoch [51/60] Train 0.000045 | Val 0.000047\n",
      "Epoch [52/60] Train 0.000044 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 52\n",
      "Epoch [53/60] Train 0.000044 | Val 0.000046\n",
      "Epoch [54/60] Train 0.000044 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 54\n",
      "Epoch [55/60] Train 0.000044 | Val 0.000048\n",
      "Epoch [56/60] Train 0.000044 | Val 0.000046\n",
      "Epoch [57/60] Train 0.000044 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 57\n",
      "Epoch [58/60] Train 0.000044 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 58\n",
      "Epoch [59/60] Train 0.000044 | Val 0.000046\n",
      "Epoch [60/60] Train 0.000044 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 60\n",
      "\n",
      "Running inference on test set...\n",
      "Predictions saved in C:\\Preet\\9000_paired_bscans\\predictions_fk_transunet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "DATASET_DIR = r\"C:\\Preet\\9000_paired_bscans\"  # *_l.png and *_h.png paired images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 60\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH = os.path.join(DATASET_DIR, \"fk_transunet++.pth\")\n",
    "RESULTS_DIR = os.path.join(DATASET_DIR, \"predictions_fk_transunet\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# f–k loss weights / options (tune if needed)\n",
    "FK_WEIGHT = 0.5           # contribution of f–k term to total loss\n",
    "FK_USE_LOG = True         # use log magnitude for stability\n",
    "FK_USE_HANN = True        # apply 2D Hann window before FFT to reduce leakage\n",
    "FK_KY = (0.05, 0.85)      # rectangular band in normalized ky (temporal freq)\n",
    "FK_KX = (0.00, 0.90)      # rectangular band in normalized kx (spatial wavenumber)\n",
    "\n",
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, x_paths, y_paths):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(Image.open(self.x_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        y = np.array(Image.open(self.y_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        x = torch.tensor(x).unsqueeze(0)  # (1,H,W)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    low_paths, high_paths = [], []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\"_l.png\"):\n",
    "            low_path = os.path.join(dataset_dir, file)\n",
    "            high_path = os.path.join(dataset_dir, file.replace(\"_l.png\", \"_h.png\"))\n",
    "            if os.path.exists(high_path):\n",
    "                low_paths.append(low_path)\n",
    "                high_paths.append(high_path)\n",
    "    return low_paths, high_paths\n",
    "\n",
    "all_x, all_y = load_data(DATASET_DIR)\n",
    "\n",
    "# Split 70/15/15\n",
    "train_x, temp_x, train_y, temp_y = train_test_split(all_x, all_y, test_size=0.30, random_state=42)\n",
    "val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_x)}, Val: {len(val_x)}, Test: {len(test_x)}\")\n",
    "\n",
    "train_loader = DataLoader(GPRDataset(train_x, train_y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(GPRDataset(val_x, val_y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(GPRDataset(test_x, test_y), batch_size=1, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None  # initialized at runtime\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)  # (B, N, patch_dim)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "# ---- NEW: CUP upsampler block ----\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Detail-preserving upsampler: conv -> pixelshuffle(2x) -> conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),                         # (H,W) -> (2H,2W)\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # ================= Encoder =================\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)            # 1 -> 64\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)        # 64 -> 128\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)      # 128 -> 256\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ================= Bottleneck =================\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "\n",
    "        # ================= Decoder (CUP) =================\n",
    "        # stage with e3 skip\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)                         # 256 -> 128 @ H/4\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)         # [128 + 256] -> 128\n",
    "        # stage with e2 skip\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)                           # 128 -> 64  @ H/2\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)             # [64 + 128] -> 64\n",
    "        # NEW: stage with e1 (high-res) skip\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)                          # 64 -> 32  @ H\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)         # [32 + 64] -> 32\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)    # 32 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Encoder ----------------\n",
    "        e1 = self.enc1(x)                       # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))           # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))           # [B,256,H/4,W/4]\n",
    "\n",
    "        # ---------------- Bottleneck ----------------\n",
    "        b = self.bottleneck(self.pool(e3))      # [B,256,H/8,W/8]\n",
    "\n",
    "        # ---------------- Decoder (CUP + all skips) ----------------\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))   # -> [B,128,H/4,W/4]\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))   # -> [B,64, H/2,W/2]\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))   # -> [B,32, H,  W]\n",
    "        out = self.out_conv(d1)                                # -> [B,1,  H,  W]\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# f–k LOSS (frequency–wavenumber consistency)\n",
    "# ==========================\n",
    "def make_rect_fk_mask(H, W, device, ky=(0.05, 0.85), kx=(0.00, 0.90)):\n",
    "    \"\"\"\n",
    "    Simple rectangular bandpass in normalized frequency space.\n",
    "    ky, kx are fractions of Nyquist in [0,1].\n",
    "    \"\"\"\n",
    "    yy = torch.linspace(-1, 1, H, device=device).abs().view(H, 1).expand(H, W)\n",
    "    xx = torch.linspace(-1, 1, W, device=device).abs().view(1, W).expand(H, W)\n",
    "    m = ((yy >= ky[0]) & (yy <= ky[1]) & (xx >= kx[0]) & (xx <= kx[1])).float()\n",
    "    return m.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "def fk_loss(pred, target, bandpass=None, p=1, use_log=True, window=True):\n",
    "    \"\"\"\n",
    "    pred/target: (B,1,H,W) in [0,1]\n",
    "    Compare spectra in f–k domain (2D FFT). L1 by default.\n",
    "    \"\"\"\n",
    "    assert pred.shape == target.shape\n",
    "    if window:\n",
    "        H, W = pred.shape[-2:]\n",
    "        wy = torch.hann_window(H, device=pred.device).view(1,1,H,1)\n",
    "        wx = torch.hann_window(W, device=pred.device).view(1,1,1,W)\n",
    "        win = wy * wx\n",
    "        pred = pred * win\n",
    "        target = target * win\n",
    "\n",
    "    P = torch.fft.fft2(pred, norm='ortho')\n",
    "    T = torch.fft.fft2(target, norm='ortho')\n",
    "    Pm = torch.abs(P)\n",
    "    Tm = torch.abs(T)\n",
    "    if use_log:\n",
    "        Pm = torch.log1p(Pm)\n",
    "        Tm = torch.log1p(Tm)\n",
    "    diff = (Pm - Tm).abs() if p == 1 else (Pm - Tm).pow(2)\n",
    "    if bandpass is not None:\n",
    "        diff = diff * bandpass.to(diff.dtype)\n",
    "    return diff.mean()\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "huber = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "\n",
    "        # Combine Huber + f–k loss (compute f–k on clamped [0,1] for stability)\n",
    "        fk_mask = make_rect_fk_mask(y.shape[2], y.shape[3], y.device, ky=FK_KY, kx=FK_KX)\n",
    "        loss_huber = huber(preds, y)\n",
    "        loss_fk = fk_loss(preds.clamp(0,1), y, bandpass=fk_mask, p=1, use_log=FK_USE_LOG, window=FK_USE_HANN)\n",
    "        loss = loss_huber + FK_WEIGHT * loss_fk\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "            fk_mask = make_rect_fk_mask(y.shape[2], y.shape[3], y.device, ky=FK_KY, kx=FK_KX)\n",
    "            loss_huber = huber(preds, y)\n",
    "            loss_fk = fk_loss(preds.clamp(0,1), y, bandpass=fk_mask, p=1, use_log=FK_USE_LOG, window=FK_USE_HANN)\n",
    "            val_loss += (loss_huber + FK_WEIGHT * loss_fk).item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train {train_loss:.6f} | Val {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  ✅ Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE\n",
    "# ==========================\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).cpu().squeeze(0).squeeze(0).numpy()\n",
    "    pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    Image.fromarray(pred_img).save(os.path.join(RESULTS_DIR, f\"pred_{i+1}.png\"))\n",
    "\n",
    "print(f\"Predictions saved in {RESULTS_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaefdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test Set Evaluation ----\n",
      "SSIM: avg=0.8933, min=0.7108, max=0.9979\n",
      "PSNR: avg=35.34 dB, min=28.53 dB, max=49.41 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "import shutil\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "RESULTS_DIR = r\"C:\\Preet\\9000_paired_bscans\\predictions_fk_transunet\"  # predictions from TransUNet\n",
    "GT_TEST_DIR = r\"C:\\Preet\\9000_paired_bscans\\ground_truth_test_fk_transunet\"\n",
    "\n",
    "# Copy ground truth test images to a dedicated folder\n",
    "os.makedirs(GT_TEST_DIR, exist_ok=True)\n",
    "for f in test_y:  # test_y comes from your train/val/test split\n",
    "    shutil.copy(f, GT_TEST_DIR)\n",
    "\n",
    "# ========================== FUNCTIONS ==========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "# ========================== MAIN EVALUATION ==========================\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "gt_files   = sorted([f for f in os.listdir(GT_TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "num_pairs = min(len(pred_files), len(gt_files))\n",
    "if num_pairs == 0:\n",
    "    print(\"[Error] No matching image files found in both directories.\")\n",
    "else:\n",
    "    if len(pred_files) != len(gt_files):\n",
    "        print(f\"[Warning] Different number of images: Predictions={len(pred_files)}, Ground Truth={len(gt_files)}\")\n",
    "        print(f\"Evaluating only first {num_pairs} matched pairs.\")\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        pred_path = os.path.join(RESULTS_DIR, pred_files[i])\n",
    "        gt_path   = os.path.join(GT_TEST_DIR, gt_files[i])\n",
    "\n",
    "        pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_img   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if pred_img is None or gt_img is None:\n",
    "            print(f\"[Error] Could not load: {pred_files[i]} or {gt_files[i]}\")\n",
    "            continue\n",
    "\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"\\n---- Test Set Evaluation ----\")\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53623348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating png_images_650M_1083M ----\n",
      "SSIM: avg=0.9504, min=0.9216, max=0.9624\n",
      "PSNR: avg=35.95 dB, min=31.54 dB, max=37.29 dB\n",
      "\n",
      "---- Evaluating png_images_700M_1167M ----\n",
      "SSIM: avg=0.9852, min=0.7560, max=0.9900\n",
      "PSNR: avg=38.55 dB, min=29.26 dB, max=39.93 dB\n",
      "\n",
      "---- Evaluating png_images_800M_1333M ----\n",
      "SSIM: avg=0.9902, min=0.9735, max=0.9938\n",
      "PSNR: avg=39.53 dB, min=34.42 dB, max=40.78 dB\n",
      "\n",
      "---- Evaluating png_images_850M_1416M ----\n",
      "SSIM: avg=0.9580, min=0.9317, max=0.9676\n",
      "PSNR: avg=34.91 dB, min=31.92 dB, max=36.29 dB\n",
      "\n",
      "---- Evaluating png_images_900M_1500M ----\n",
      "SSIM: avg=0.9242, min=0.8702, max=0.9424\n",
      "PSNR: avg=32.61 dB, min=29.65 dB, max=34.51 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FREQ_DIRS = [\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_650M_1083M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_700M_1167M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_800M_1333M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_850M_1416M\",\n",
    "    r\"C:\\Preet\\validation dataset\\png_images_900M_1500M\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\9000_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE LOOP\n",
    "# =========================\n",
    "for freq_dir in FREQ_DIRS:\n",
    "    print(f\"\\n---- Evaluating {os.path.basename(freq_dir)} ----\")\n",
    "    pred_dir = os.path.join(freq_dir, \"predictions_fk_transunet\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    low_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_l.png\")])\n",
    "    high_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_h.png\")])\n",
    "\n",
    "    psnr_values, ssim_values = [], []\n",
    "\n",
    "    for i in range(len(low_files)):\n",
    "        low_path = os.path.join(freq_dir, low_files[i])\n",
    "        high_path = os.path.join(freq_dir, high_files[i])\n",
    "\n",
    "        # Load LR\n",
    "        lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = model(lr_tensor).cpu().squeeze().numpy()\n",
    "\n",
    "        # Scale to 0-255\n",
    "        pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "\n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "        Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "        # Load GT\n",
    "        gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        # Metrics\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "    else:\n",
    "        print(\"[Error] No valid pairs processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10fe076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 650 → pred → compare with 1083 ----\n",
      "SSIM: avg=0.9478, min=0.9217, max=0.9610\n",
      "PSNR: avg=27.36 dB, min=26.05 dB, max=27.78 dB\n",
      "\n",
      "---- Stage 2: pred_1083 → pred → compare with 1800 ----\n",
      "SSIM: avg=0.9347, min=0.8929, max=0.9553\n",
      "PSNR: avg=27.32 dB, min=23.13 dB, max=28.62 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset\\png_images_650M_1083M_1800M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\9000_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    \"\"\"Runs inference with proper scaling and output normalization\"\"\"\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    # scale back to [0,255]\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    pred = (pred * 255.0).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"650_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1083_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1800_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_fk_transunet\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_fk_transunet\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32) / 255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 650 → pred → compare with 1083 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1083 → pred → compare with 1800 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8205eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 900 → pred → compare with 1500 ----\n",
      "SSIM: avg=0.7180, min=0.5728, max=0.8518\n",
      "PSNR: avg=17.10 dB, min=15.76 dB, max=17.82 dB\n",
      "\n",
      "---- Stage 2: pred_1500 → pred → compare with 2500 ----\n",
      "SSIM: avg=0.7242, min=0.6003, max=0.8451\n",
      "PSNR: avg=15.46 dB, min=14.46 dB, max=15.99 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset\\png_images_900M_1500M_2500M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\9000_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    \"\"\"Runs inference with proper scaling and output normalization\"\"\"\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    # scale back to [0,255]\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    pred = (pred * 255.0).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"900_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1500_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"2500_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_fk_transunet\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_fk_transunet\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32) / 255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 900 → pred → compare with 1500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1500 → pred → compare with 2500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d0afb",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343e101",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec44f2",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff4db7",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b6816",
   "metadata": {},
   "source": [
    "4000 paired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ee7573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Train: 2800, Val: 600, Test: 600\n",
      "Epoch [1/60] Train 0.005991 | Val 0.000253\n",
      "  ✅ Saved Best Model at Epoch 1\n",
      "Epoch [2/60] Train 0.000200 | Val 0.000150\n",
      "  ✅ Saved Best Model at Epoch 2\n",
      "Epoch [3/60] Train 0.000142 | Val 0.000131\n",
      "  ✅ Saved Best Model at Epoch 3\n",
      "Epoch [4/60] Train 0.000121 | Val 0.000116\n",
      "  ✅ Saved Best Model at Epoch 4\n",
      "Epoch [5/60] Train 0.000113 | Val 0.000107\n",
      "  ✅ Saved Best Model at Epoch 5\n",
      "Epoch [6/60] Train 0.000105 | Val 0.000102\n",
      "  ✅ Saved Best Model at Epoch 6\n",
      "Epoch [7/60] Train 0.000102 | Val 0.000097\n",
      "  ✅ Saved Best Model at Epoch 7\n",
      "Epoch [8/60] Train 0.000097 | Val 0.000118\n",
      "Epoch [9/60] Train 0.000093 | Val 0.000093\n",
      "  ✅ Saved Best Model at Epoch 9\n",
      "Epoch [10/60] Train 0.000092 | Val 0.000089\n",
      "  ✅ Saved Best Model at Epoch 10\n",
      "Epoch [11/60] Train 0.000089 | Val 0.000088\n",
      "  ✅ Saved Best Model at Epoch 11\n",
      "Epoch [12/60] Train 0.000088 | Val 0.000099\n",
      "Epoch [13/60] Train 0.000082 | Val 0.000085\n",
      "  ✅ Saved Best Model at Epoch 13\n",
      "Epoch [14/60] Train 0.000080 | Val 0.000099\n",
      "Epoch [15/60] Train 0.000077 | Val 0.000075\n",
      "  ✅ Saved Best Model at Epoch 15\n",
      "Epoch [16/60] Train 0.000076 | Val 0.000074\n",
      "  ✅ Saved Best Model at Epoch 16\n",
      "Epoch [17/60] Train 0.000073 | Val 0.000078\n",
      "Epoch [18/60] Train 0.000071 | Val 0.000067\n",
      "  ✅ Saved Best Model at Epoch 18\n",
      "Epoch [19/60] Train 0.000070 | Val 0.000070\n",
      "Epoch [20/60] Train 0.000067 | Val 0.000069\n",
      "Epoch [21/60] Train 0.000068 | Val 0.000067\n",
      "  ✅ Saved Best Model at Epoch 21\n",
      "Epoch [22/60] Train 0.000066 | Val 0.000062\n",
      "  ✅ Saved Best Model at Epoch 22\n",
      "Epoch [23/60] Train 0.000066 | Val 0.000064\n",
      "Epoch [24/60] Train 0.000062 | Val 0.000066\n",
      "Epoch [25/60] Train 0.000063 | Val 0.000062\n",
      "  ✅ Saved Best Model at Epoch 25\n",
      "Epoch [26/60] Train 0.000060 | Val 0.000061\n",
      "  ✅ Saved Best Model at Epoch 26\n",
      "Epoch [27/60] Train 0.000060 | Val 0.000059\n",
      "  ✅ Saved Best Model at Epoch 27\n",
      "Epoch [28/60] Train 0.000058 | Val 0.000059\n",
      "Epoch [29/60] Train 0.000059 | Val 0.000059\n",
      "  ✅ Saved Best Model at Epoch 29\n",
      "Epoch [30/60] Train 0.000057 | Val 0.000072\n",
      "Epoch [31/60] Train 0.000057 | Val 0.000057\n",
      "  ✅ Saved Best Model at Epoch 31\n",
      "Epoch [32/60] Train 0.000058 | Val 0.000057\n",
      "  ✅ Saved Best Model at Epoch 32\n",
      "Epoch [33/60] Train 0.000056 | Val 0.000062\n",
      "Epoch [34/60] Train 0.000057 | Val 0.000057\n",
      "Epoch [35/60] Train 0.000054 | Val 0.000057\n",
      "  ✅ Saved Best Model at Epoch 35\n",
      "Epoch [36/60] Train 0.000054 | Val 0.000058\n",
      "Epoch [37/60] Train 0.000054 | Val 0.000058\n",
      "Epoch [38/60] Train 0.000054 | Val 0.000059\n",
      "Epoch [39/60] Train 0.000054 | Val 0.000056\n",
      "  ✅ Saved Best Model at Epoch 39\n",
      "Epoch [40/60] Train 0.000054 | Val 0.000057\n",
      "Epoch [41/60] Train 0.000053 | Val 0.000061\n",
      "Epoch [42/60] Train 0.000053 | Val 0.000061\n",
      "Epoch [43/60] Train 0.000053 | Val 0.000055\n",
      "  ✅ Saved Best Model at Epoch 43\n",
      "Epoch [44/60] Train 0.000051 | Val 0.000058\n",
      "Epoch [45/60] Train 0.000051 | Val 0.000055\n",
      "Epoch [46/60] Train 0.000051 | Val 0.000054\n",
      "  ✅ Saved Best Model at Epoch 46\n",
      "Epoch [47/60] Train 0.000051 | Val 0.000056\n",
      "Epoch [48/60] Train 0.000051 | Val 0.000057\n",
      "Epoch [49/60] Train 0.000051 | Val 0.000054\n",
      "Epoch [50/60] Train 0.000050 | Val 0.000054\n",
      "Epoch [51/60] Train 0.000050 | Val 0.000057\n",
      "Epoch [52/60] Train 0.000049 | Val 0.000057\n",
      "Epoch [53/60] Train 0.000050 | Val 0.000054\n",
      "Epoch [54/60] Train 0.000049 | Val 0.000055\n",
      "Epoch [55/60] Train 0.000048 | Val 0.000053\n",
      "  ✅ Saved Best Model at Epoch 55\n",
      "Epoch [56/60] Train 0.000048 | Val 0.000052\n",
      "  ✅ Saved Best Model at Epoch 56\n",
      "Epoch [57/60] Train 0.000048 | Val 0.000054\n",
      "Epoch [58/60] Train 0.000049 | Val 0.000052\n",
      "Epoch [59/60] Train 0.000048 | Val 0.000053\n",
      "Epoch [60/60] Train 0.000049 | Val 0.000053\n",
      "\n",
      "Running inference on test set...\n",
      "Predictions saved in C:\\Preet\\4000_paired_bscans\\predictions_fk_transunet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "DATASET_DIR = r\"C:\\Preet\\4000_paired_bscans\"  # *_l.png and *_h.png paired images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 60\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH = os.path.join(DATASET_DIR, \"fk_transunet++.pth\")\n",
    "RESULTS_DIR = os.path.join(DATASET_DIR, \"predictions_fk_transunet\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# f–k loss weights / options (tune if needed)\n",
    "FK_WEIGHT = 0.5           # contribution of f–k term to total loss\n",
    "FK_USE_LOG = True         # use log magnitude for stability\n",
    "FK_USE_HANN = True        # apply 2D Hann window before FFT to reduce leakage\n",
    "FK_KY = (0.05, 0.85)      # rectangular band in normalized ky (temporal freq)\n",
    "FK_KX = (0.00, 0.90)      # rectangular band in normalized kx (spatial wavenumber)\n",
    "\n",
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, x_paths, y_paths):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(Image.open(self.x_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        y = np.array(Image.open(self.y_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        x = torch.tensor(x).unsqueeze(0)  # (1,H,W)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    low_paths, high_paths = [], []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\"_l.png\"):\n",
    "            low_path = os.path.join(dataset_dir, file)\n",
    "            high_path = os.path.join(dataset_dir, file.replace(\"_l.png\", \"_h.png\"))\n",
    "            if os.path.exists(high_path):\n",
    "                low_paths.append(low_path)\n",
    "                high_paths.append(high_path)\n",
    "    return low_paths, high_paths\n",
    "\n",
    "all_x, all_y = load_data(DATASET_DIR)\n",
    "\n",
    "# Split 70/15/15\n",
    "train_x, temp_x, train_y, temp_y = train_test_split(all_x, all_y, test_size=0.30, random_state=42)\n",
    "val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_x)}, Val: {len(val_x)}, Test: {len(test_x)}\")\n",
    "\n",
    "train_loader = DataLoader(GPRDataset(train_x, train_y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(GPRDataset(val_x, val_y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(GPRDataset(test_x, test_y), batch_size=1, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None  # initialized at runtime\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)  # (B, N, patch_dim)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "# ---- NEW: CUP upsampler block ----\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Detail-preserving upsampler: conv -> pixelshuffle(2x) -> conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),                         # (H,W) -> (2H,2W)\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # ================= Encoder =================\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)            # 1 -> 64\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)        # 64 -> 128\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)      # 128 -> 256\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ================= Bottleneck =================\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "\n",
    "        # ================= Decoder (CUP) =================\n",
    "        # stage with e3 skip\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)                         # 256 -> 128 @ H/4\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)         # [128 + 256] -> 128\n",
    "        # stage with e2 skip\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)                           # 128 -> 64  @ H/2\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)             # [64 + 128] -> 64\n",
    "        # NEW: stage with e1 (high-res) skip\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)                          # 64 -> 32  @ H\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)         # [32 + 64] -> 32\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)    # 32 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Encoder ----------------\n",
    "        e1 = self.enc1(x)                       # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))           # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))           # [B,256,H/4,W/4]\n",
    "\n",
    "        # ---------------- Bottleneck ----------------\n",
    "        b = self.bottleneck(self.pool(e3))      # [B,256,H/8,W/8]\n",
    "\n",
    "        # ---------------- Decoder (CUP + all skips) ----------------\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))   # -> [B,128,H/4,W/4]\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))   # -> [B,64, H/2,W/2]\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))   # -> [B,32, H,  W]\n",
    "        out = self.out_conv(d1)                                # -> [B,1,  H,  W]\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# f–k LOSS (frequency–wavenumber consistency)\n",
    "# ==========================\n",
    "def make_rect_fk_mask(H, W, device, ky=(0.05, 0.85), kx=(0.00, 0.90)):\n",
    "    \"\"\"\n",
    "    Simple rectangular bandpass in normalized frequency space.\n",
    "    ky, kx are fractions of Nyquist in [0,1].\n",
    "    \"\"\"\n",
    "    yy = torch.linspace(-1, 1, H, device=device).abs().view(H, 1).expand(H, W)\n",
    "    xx = torch.linspace(-1, 1, W, device=device).abs().view(1, W).expand(H, W)\n",
    "    m = ((yy >= ky[0]) & (yy <= ky[1]) & (xx >= kx[0]) & (xx <= kx[1])).float()\n",
    "    return m.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "def fk_loss(pred, target, bandpass=None, p=1, use_log=True, window=True):\n",
    "    \"\"\"\n",
    "    pred/target: (B,1,H,W) in [0,1]\n",
    "    Compare spectra in f–k domain (2D FFT). L1 by default.\n",
    "    \"\"\"\n",
    "    assert pred.shape == target.shape\n",
    "    if window:\n",
    "        H, W = pred.shape[-2:]\n",
    "        wy = torch.hann_window(H, device=pred.device).view(1,1,H,1)\n",
    "        wx = torch.hann_window(W, device=pred.device).view(1,1,1,W)\n",
    "        win = wy * wx\n",
    "        pred = pred * win\n",
    "        target = target * win\n",
    "\n",
    "    P = torch.fft.fft2(pred, norm='ortho')\n",
    "    T = torch.fft.fft2(target, norm='ortho')\n",
    "    Pm = torch.abs(P)\n",
    "    Tm = torch.abs(T)\n",
    "    if use_log:\n",
    "        Pm = torch.log1p(Pm)\n",
    "        Tm = torch.log1p(Tm)\n",
    "    diff = (Pm - Tm).abs() if p == 1 else (Pm - Tm).pow(2)\n",
    "    if bandpass is not None:\n",
    "        diff = diff * bandpass.to(diff.dtype)\n",
    "    return diff.mean()\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "huber = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "\n",
    "        # Combine Huber + f–k loss (compute f–k on clamped [0,1] for stability)\n",
    "        fk_mask = make_rect_fk_mask(y.shape[2], y.shape[3], y.device, ky=FK_KY, kx=FK_KX)\n",
    "        loss_huber = huber(preds, y)\n",
    "        loss_fk = fk_loss(preds.clamp(0,1), y, bandpass=fk_mask, p=1, use_log=FK_USE_LOG, window=FK_USE_HANN)\n",
    "        loss = loss_huber + FK_WEIGHT * loss_fk\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "            fk_mask = make_rect_fk_mask(y.shape[2], y.shape[3], y.device, ky=FK_KY, kx=FK_KX)\n",
    "            loss_huber = huber(preds, y)\n",
    "            loss_fk = fk_loss(preds.clamp(0,1), y, bandpass=fk_mask, p=1, use_log=FK_USE_LOG, window=FK_USE_HANN)\n",
    "            val_loss += (loss_huber + FK_WEIGHT * loss_fk).item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train {train_loss:.6f} | Val {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  ✅ Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE\n",
    "# ==========================\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).cpu().squeeze(0).squeeze(0).numpy()\n",
    "    pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    Image.fromarray(pred_img).save(os.path.join(RESULTS_DIR, f\"pred_{i+1}.png\"))\n",
    "\n",
    "print(f\"Predictions saved in {RESULTS_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c55b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test Set Evaluation ----\n",
      "SSIM: avg=0.8918, min=0.7129, max=0.9971\n",
      "PSNR: avg=35.42 dB, min=28.93 dB, max=51.88 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "import shutil\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "RESULTS_DIR = r\"C:\\Preet\\4000_paired_bscans\\predictions_fk_transunet\"  # predictions from TransUNet\n",
    "GT_TEST_DIR = r\"C:\\Preet\\4000_paired_bscans\\ground_truth_test_fk_transunet\"\n",
    "\n",
    "# Copy ground truth test images to a dedicated folder\n",
    "os.makedirs(GT_TEST_DIR, exist_ok=True)\n",
    "for f in test_y:  # test_y comes from your train/val/test split\n",
    "    shutil.copy(f, GT_TEST_DIR)\n",
    "\n",
    "# ========================== FUNCTIONS ==========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "# ========================== MAIN EVALUATION ==========================\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "gt_files   = sorted([f for f in os.listdir(GT_TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "num_pairs = min(len(pred_files), len(gt_files))\n",
    "if num_pairs == 0:\n",
    "    print(\"[Error] No matching image files found in both directories.\")\n",
    "else:\n",
    "    if len(pred_files) != len(gt_files):\n",
    "        print(f\"[Warning] Different number of images: Predictions={len(pred_files)}, Ground Truth={len(gt_files)}\")\n",
    "        print(f\"Evaluating only first {num_pairs} matched pairs.\")\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        pred_path = os.path.join(RESULTS_DIR, pred_files[i])\n",
    "        gt_path   = os.path.join(GT_TEST_DIR, gt_files[i])\n",
    "\n",
    "        pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_img   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if pred_img is None or gt_img is None:\n",
    "            print(f\"[Error] Could not load: {pred_files[i]} or {gt_files[i]}\")\n",
    "            continue\n",
    "\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"\\n---- Test Set Evaluation ----\")\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f72cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating png_images_650M_1083M ----\n",
      "SSIM: avg=0.9480, min=0.9125, max=0.9604\n",
      "PSNR: avg=35.70 dB, min=31.81 dB, max=37.01 dB\n",
      "\n",
      "---- Evaluating png_images_700M_1167M ----\n",
      "SSIM: avg=0.9816, min=0.7558, max=0.9873\n",
      "PSNR: avg=37.80 dB, min=29.25 dB, max=38.87 dB\n",
      "\n",
      "---- Evaluating png_images_800M_1333M ----\n",
      "SSIM: avg=0.9896, min=0.9652, max=0.9935\n",
      "PSNR: avg=38.79 dB, min=33.71 dB, max=39.78 dB\n",
      "\n",
      "---- Evaluating png_images_850M_1416M ----\n",
      "SSIM: avg=0.9661, min=0.9283, max=0.9761\n",
      "PSNR: avg=34.76 dB, min=31.21 dB, max=35.74 dB\n",
      "\n",
      "---- Evaluating png_images_900M_1500M ----\n",
      "SSIM: avg=0.9314, min=0.8686, max=0.9496\n",
      "PSNR: avg=33.14 dB, min=29.57 dB, max=34.01 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FREQ_DIRS = [\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_650M_1083M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_700M_1167M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_800M_1333M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_850M_1416M\",\n",
    "    r\"C:\\Preet\\validation dataset_4000\\png_images_900M_1500M\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\4000_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE LOOP\n",
    "# =========================\n",
    "for freq_dir in FREQ_DIRS:\n",
    "    print(f\"\\n---- Evaluating {os.path.basename(freq_dir)} ----\")\n",
    "    pred_dir = os.path.join(freq_dir, \"predictions_fk_transunet\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    low_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_l.png\")])\n",
    "    high_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_h.png\")])\n",
    "\n",
    "    psnr_values, ssim_values = [], []\n",
    "\n",
    "    for i in range(len(low_files)):\n",
    "        low_path = os.path.join(freq_dir, low_files[i])\n",
    "        high_path = os.path.join(freq_dir, high_files[i])\n",
    "\n",
    "        # Load LR\n",
    "        lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = model(lr_tensor).cpu().squeeze().numpy()\n",
    "\n",
    "        # Scale to 0-255\n",
    "        pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "\n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "        Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "        # Load GT\n",
    "        gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        # Metrics\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "    else:\n",
    "        print(\"[Error] No valid pairs processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4e83d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 650 → pred → compare with 1083 ----\n",
      "SSIM: avg=0.9457, min=0.9229, max=0.9597\n",
      "PSNR: avg=27.56 dB, min=25.87 dB, max=28.01 dB\n",
      "\n",
      "---- Stage 2: pred_1083 → pred → compare with 1800 ----\n",
      "SSIM: avg=0.9331, min=0.8888, max=0.9526\n",
      "PSNR: avg=26.33 dB, min=24.57 dB, max=26.99 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset_4000\\png_images_650M_1083M_1800M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\4000_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    \"\"\"Runs inference with proper scaling and output normalization\"\"\"\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    # scale back to [0,255]\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    pred = (pred * 255.0).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"650_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1083_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1800_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_fk_transunet\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_fk_transunet\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32) / 255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 650 → pred → compare with 1083 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1083 → pred → compare with 1800 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db5b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 900 → pred → compare with 1500 ----\n",
      "SSIM: avg=0.7392, min=0.5940, max=0.8697\n",
      "PSNR: avg=17.32 dB, min=15.82 dB, max=18.08 dB\n",
      "\n",
      "---- Stage 2: pred_1500 → pred → compare with 2500 ----\n",
      "SSIM: avg=0.7404, min=0.6277, max=0.8415\n",
      "PSNR: avg=15.55 dB, min=14.07 dB, max=16.20 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\validation dataset_4000\\png_images_900M_1500M_2500M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\4000_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    \"\"\"Runs inference with proper scaling and output normalization\"\"\"\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    # scale back to [0,255]\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    pred = (pred * 255.0).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"900_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1500_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"2500_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_fk_transunet\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_fk_transunet\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32) / 255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 900 → pred → compare with 1500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1500 → pred → compare with 2500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2510ecb",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac583c",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d149e58",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a2a40",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc67aa",
   "metadata": {},
   "source": [
    "clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc66413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Train: 5810, Val: 1245, Test: 1245\n",
      "Epoch [1/75] Train 0.004010 | Val 0.000176\n",
      "  ✅ Saved Best Model at Epoch 1\n",
      "Epoch [2/75] Train 0.000105 | Val 0.000086\n",
      "  ✅ Saved Best Model at Epoch 2\n",
      "Epoch [3/75] Train 0.000084 | Val 0.000077\n",
      "  ✅ Saved Best Model at Epoch 3\n",
      "Epoch [4/75] Train 0.000077 | Val 0.000072\n",
      "  ✅ Saved Best Model at Epoch 4\n",
      "Epoch [5/75] Train 0.000075 | Val 0.000069\n",
      "  ✅ Saved Best Model at Epoch 5\n",
      "Epoch [6/75] Train 0.000072 | Val 0.000067\n",
      "  ✅ Saved Best Model at Epoch 6\n",
      "Epoch [7/75] Train 0.000070 | Val 0.000064\n",
      "  ✅ Saved Best Model at Epoch 7\n",
      "Epoch [8/75] Train 0.000066 | Val 0.000068\n",
      "Epoch [9/75] Train 0.000062 | Val 0.000064\n",
      "  ✅ Saved Best Model at Epoch 9\n",
      "Epoch [10/75] Train 0.000055 | Val 0.000053\n",
      "  ✅ Saved Best Model at Epoch 10\n",
      "Epoch [11/75] Train 0.000052 | Val 0.000049\n",
      "  ✅ Saved Best Model at Epoch 11\n",
      "Epoch [12/75] Train 0.000051 | Val 0.000049\n",
      "Epoch [13/75] Train 0.000050 | Val 0.000058\n",
      "Epoch [14/75] Train 0.000050 | Val 0.000059\n",
      "Epoch [15/75] Train 0.000049 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 15\n",
      "Epoch [16/75] Train 0.000048 | Val 0.000048\n",
      "Epoch [17/75] Train 0.000048 | Val 0.000053\n",
      "Epoch [18/75] Train 0.000048 | Val 0.000048\n",
      "Epoch [19/75] Train 0.000047 | Val 0.000046\n",
      "  ✅ Saved Best Model at Epoch 19\n",
      "Epoch [20/75] Train 0.000047 | Val 0.000047\n",
      "Epoch [21/75] Train 0.000047 | Val 0.000048\n",
      "Epoch [22/75] Train 0.000047 | Val 0.000045\n",
      "  ✅ Saved Best Model at Epoch 22\n",
      "Epoch [23/75] Train 0.000046 | Val 0.000047\n",
      "Epoch [24/75] Train 0.000046 | Val 0.000046\n",
      "Epoch [25/75] Train 0.000046 | Val 0.000072\n",
      "Epoch [26/75] Train 0.000046 | Val 0.000047\n",
      "Epoch [27/75] Train 0.000045 | Val 0.000044\n",
      "  ✅ Saved Best Model at Epoch 27\n",
      "Epoch [28/75] Train 0.000045 | Val 0.000044\n",
      "Epoch [29/75] Train 0.000045 | Val 0.000044\n",
      "  ✅ Saved Best Model at Epoch 29\n",
      "Epoch [30/75] Train 0.000045 | Val 0.000046\n",
      "Epoch [31/75] Train 0.000044 | Val 0.000045\n",
      "Epoch [32/75] Train 0.000044 | Val 0.000044\n",
      "Epoch [33/75] Train 0.000044 | Val 0.000045\n",
      "Epoch [34/75] Train 0.000044 | Val 0.000044\n",
      "Epoch [35/75] Train 0.000044 | Val 0.000046\n",
      "Epoch [36/75] Train 0.000043 | Val 0.000044\n",
      "Epoch [37/75] Train 0.000043 | Val 0.000078\n",
      "Epoch [38/75] Train 0.000044 | Val 0.000046\n",
      "Epoch [39/75] Train 0.000043 | Val 0.000043\n",
      "  ✅ Saved Best Model at Epoch 39\n",
      "Epoch [40/75] Train 0.000043 | Val 0.000043\n",
      "  ✅ Saved Best Model at Epoch 40\n",
      "Epoch [41/75] Train 0.000043 | Val 0.000043\n",
      "  ✅ Saved Best Model at Epoch 41\n",
      "Epoch [42/75] Train 0.000043 | Val 0.000043\n",
      "Epoch [43/75] Train 0.000043 | Val 0.000043\n",
      "  ✅ Saved Best Model at Epoch 43\n",
      "Epoch [44/75] Train 0.000042 | Val 0.000043\n",
      "Epoch [45/75] Train 0.000042 | Val 0.000043\n",
      "  ✅ Saved Best Model at Epoch 45\n",
      "Epoch [46/75] Train 0.000042 | Val 0.000042\n",
      "  ✅ Saved Best Model at Epoch 46\n",
      "Epoch [47/75] Train 0.000042 | Val 0.000043\n",
      "Epoch [48/75] Train 0.000042 | Val 0.000042\n",
      "Epoch [49/75] Train 0.000042 | Val 0.000043\n",
      "Epoch [50/75] Train 0.000042 | Val 0.000042\n",
      "Epoch [51/75] Train 0.000042 | Val 0.000045\n",
      "Epoch [52/75] Train 0.000042 | Val 0.000043\n",
      "Epoch [53/75] Train 0.000042 | Val 0.000043\n",
      "Epoch [54/75] Train 0.000042 | Val 0.000042\n",
      "  ✅ Saved Best Model at Epoch 54\n",
      "Epoch [55/75] Train 0.000041 | Val 0.000042\n",
      "  ✅ Saved Best Model at Epoch 55\n",
      "Epoch [56/75] Train 0.000042 | Val 0.000042\n",
      "  ✅ Saved Best Model at Epoch 56\n",
      "Epoch [57/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [58/75] Train 0.000042 | Val 0.000042\n",
      "Epoch [59/75] Train 0.000041 | Val 0.000043\n",
      "Epoch [60/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [61/75] Train 0.000041 | Val 0.000042\n",
      "  ✅ Saved Best Model at Epoch 61\n",
      "Epoch [62/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [63/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [64/75] Train 0.000041 | Val 0.000045\n",
      "Epoch [65/75] Train 0.000041 | Val 0.000043\n",
      "Epoch [66/75] Train 0.000041 | Val 0.000046\n",
      "Epoch [67/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [68/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [69/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [70/75] Train 0.000041 | Val 0.000044\n",
      "Epoch [71/75] Train 0.000041 | Val 0.000042\n",
      "  ✅ Saved Best Model at Epoch 71\n",
      "Epoch [72/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [73/75] Train 0.000041 | Val 0.000046\n",
      "Epoch [74/75] Train 0.000041 | Val 0.000042\n",
      "Epoch [75/75] Train 0.000041 | Val 0.000044\n",
      "\n",
      "Running inference on test set...\n",
      "Predictions saved in C:\\Preet\\clean_paired_bscans\\predictions_fk_transunet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "DATASET_DIR = r\"C:\\Preet\\clean_paired_bscans\"  # *_l.png and *_h.png paired images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 75\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH = os.path.join(DATASET_DIR, \"fk_transunet++.pth\")\n",
    "RESULTS_DIR = os.path.join(DATASET_DIR, \"predictions_fk_transunet\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# f–k loss weights / options (tune if needed)\n",
    "FK_WEIGHT = 0.5           # contribution of f–k term to total loss\n",
    "FK_USE_LOG = True         # use log magnitude for stability\n",
    "FK_USE_HANN = True        # apply 2D Hann window before FFT to reduce leakage\n",
    "FK_KY = (0.05, 0.85)      # rectangular band in normalized ky (temporal freq)\n",
    "FK_KX = (0.00, 0.90)      # rectangular band in normalized kx (spatial wavenumber)\n",
    "\n",
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, x_paths, y_paths):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(Image.open(self.x_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        y = np.array(Image.open(self.y_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        x = torch.tensor(x).unsqueeze(0)  # (1,H,W)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    low_paths, high_paths = [], []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\"_l.png\"):\n",
    "            low_path = os.path.join(dataset_dir, file)\n",
    "            high_path = os.path.join(dataset_dir, file.replace(\"_l.png\", \"_h.png\"))\n",
    "            if os.path.exists(high_path):\n",
    "                low_paths.append(low_path)\n",
    "                high_paths.append(high_path)\n",
    "    return low_paths, high_paths\n",
    "\n",
    "all_x, all_y = load_data(DATASET_DIR)\n",
    "\n",
    "# Split 70/15/15\n",
    "train_x, temp_x, train_y, temp_y = train_test_split(all_x, all_y, test_size=0.30, random_state=42)\n",
    "val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_x)}, Val: {len(val_x)}, Test: {len(test_x)}\")\n",
    "\n",
    "train_loader = DataLoader(GPRDataset(train_x, train_y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(GPRDataset(val_x, val_y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(GPRDataset(test_x, test_y), batch_size=1, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None  # initialized at runtime\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)  # (B, N, patch_dim)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "# ---- NEW: CUP upsampler block ----\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Detail-preserving upsampler: conv -> pixelshuffle(2x) -> conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),                         # (H,W) -> (2H,2W)\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # ================= Encoder =================\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)            # 1 -> 64\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)        # 64 -> 128\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)      # 128 -> 256\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ================= Bottleneck =================\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "\n",
    "        # ================= Decoder (CUP) =================\n",
    "        # stage with e3 skip\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)                         # 256 -> 128 @ H/4\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)         # [128 + 256] -> 128\n",
    "        # stage with e2 skip\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)                           # 128 -> 64  @ H/2\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)             # [64 + 128] -> 64\n",
    "        # NEW: stage with e1 (high-res) skip\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)                          # 64 -> 32  @ H\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)         # [32 + 64] -> 32\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)    # 32 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Encoder ----------------\n",
    "        e1 = self.enc1(x)                       # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))           # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))           # [B,256,H/4,W/4]\n",
    "\n",
    "        # ---------------- Bottleneck ----------------\n",
    "        b = self.bottleneck(self.pool(e3))      # [B,256,H/8,W/8]\n",
    "\n",
    "        # ---------------- Decoder (CUP + all skips) ----------------\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))   # -> [B,128,H/4,W/4]\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))   # -> [B,64, H/2,W/2]\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))   # -> [B,32, H,  W]\n",
    "        out = self.out_conv(d1)                                # -> [B,1,  H,  W]\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# f–k LOSS (frequency–wavenumber consistency)\n",
    "# ==========================\n",
    "def make_rect_fk_mask(H, W, device, ky=(0.05, 0.85), kx=(0.00, 0.90)):\n",
    "    \"\"\"\n",
    "    Simple rectangular bandpass in normalized frequency space.\n",
    "    ky, kx are fractions of Nyquist in [0,1].\n",
    "    \"\"\"\n",
    "    yy = torch.linspace(-1, 1, H, device=device).abs().view(H, 1).expand(H, W)\n",
    "    xx = torch.linspace(-1, 1, W, device=device).abs().view(1, W).expand(H, W)\n",
    "    m = ((yy >= ky[0]) & (yy <= ky[1]) & (xx >= kx[0]) & (xx <= kx[1])).float()\n",
    "    return m.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "def fk_loss(pred, target, bandpass=None, p=1, use_log=True, window=True):\n",
    "    \"\"\"\n",
    "    pred/target: (B,1,H,W) in [0,1]\n",
    "    Compare spectra in f–k domain (2D FFT). L1 by default.\n",
    "    \"\"\"\n",
    "    assert pred.shape == target.shape\n",
    "    if window:\n",
    "        H, W = pred.shape[-2:]\n",
    "        wy = torch.hann_window(H, device=pred.device).view(1,1,H,1)\n",
    "        wx = torch.hann_window(W, device=pred.device).view(1,1,1,W)\n",
    "        win = wy * wx\n",
    "        pred = pred * win\n",
    "        target = target * win\n",
    "\n",
    "    P = torch.fft.fft2(pred, norm='ortho')\n",
    "    T = torch.fft.fft2(target, norm='ortho')\n",
    "    Pm = torch.abs(P)\n",
    "    Tm = torch.abs(T)\n",
    "    if use_log:\n",
    "        Pm = torch.log1p(Pm)\n",
    "        Tm = torch.log1p(Tm)\n",
    "    diff = (Pm - Tm).abs() if p == 1 else (Pm - Tm).pow(2)\n",
    "    if bandpass is not None:\n",
    "        diff = diff * bandpass.to(diff.dtype)\n",
    "    return diff.mean()\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "huber = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "\n",
    "        # Combine Huber + f–k loss (compute f–k on clamped [0,1] for stability)\n",
    "        fk_mask = make_rect_fk_mask(y.shape[2], y.shape[3], y.device, ky=FK_KY, kx=FK_KX)\n",
    "        loss_huber = huber(preds, y)\n",
    "        loss_fk = fk_loss(preds.clamp(0,1), y, bandpass=fk_mask, p=1, use_log=FK_USE_LOG, window=FK_USE_HANN)\n",
    "        loss = loss_huber + FK_WEIGHT * loss_fk\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x)\n",
    "            fk_mask = make_rect_fk_mask(y.shape[2], y.shape[3], y.device, ky=FK_KY, kx=FK_KX)\n",
    "            loss_huber = huber(preds, y)\n",
    "            loss_fk = fk_loss(preds.clamp(0,1), y, bandpass=fk_mask, p=1, use_log=FK_USE_LOG, window=FK_USE_HANN)\n",
    "            val_loss += (loss_huber + FK_WEIGHT * loss_fk).item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train {train_loss:.6f} | Val {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  ✅ Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE\n",
    "# ==========================\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    x = x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).cpu().squeeze(0).squeeze(0).numpy()\n",
    "    pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    Image.fromarray(pred_img).save(os.path.join(RESULTS_DIR, f\"pred_{i+1}.png\"))\n",
    "\n",
    "print(f\"Predictions saved in {RESULTS_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb99407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test Set Evaluation ----\n",
      "SSIM: avg=0.9052, min=0.7496, max=0.9984\n",
      "PSNR: avg=35.91 dB, min=29.22 dB, max=49.25 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "import shutil\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "RESULTS_DIR = r\"C:\\Preet\\clean_paired_bscans\\predictions_fk_transunet\"  # predictions from TransUNet\n",
    "GT_TEST_DIR = r\"C:\\Preet\\clean_paired_bscans\\ground_truth_test_fk_transunet\"\n",
    "\n",
    "# Copy ground truth test images to a dedicated folder\n",
    "os.makedirs(GT_TEST_DIR, exist_ok=True)\n",
    "for f in test_y:  # test_y comes from your train/val/test split\n",
    "    shutil.copy(f, GT_TEST_DIR)\n",
    "\n",
    "# ========================== FUNCTIONS ==========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "# ========================== MAIN EVALUATION ==========================\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "gt_files   = sorted([f for f in os.listdir(GT_TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "num_pairs = min(len(pred_files), len(gt_files))\n",
    "if num_pairs == 0:\n",
    "    print(\"[Error] No matching image files found in both directories.\")\n",
    "else:\n",
    "    if len(pred_files) != len(gt_files):\n",
    "        print(f\"[Warning] Different number of images: Predictions={len(pred_files)}, Ground Truth={len(gt_files)}\")\n",
    "        print(f\"Evaluating only first {num_pairs} matched pairs.\")\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        pred_path = os.path.join(RESULTS_DIR, pred_files[i])\n",
    "        gt_path   = os.path.join(GT_TEST_DIR, gt_files[i])\n",
    "\n",
    "        pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_img   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if pred_img is None or gt_img is None:\n",
    "            print(f\"[Error] Could not load: {pred_files[i]} or {gt_files[i]}\")\n",
    "            continue\n",
    "\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"\\n---- Test Set Evaluation ----\")\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90a7ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating png_images_650M_1083M ----\n",
      "SSIM: avg=0.9488, min=0.9197, max=0.9592\n",
      "PSNR: avg=36.03 dB, min=31.83 dB, max=37.06 dB\n",
      "\n",
      "---- Evaluating png_images_700M_1167M ----\n",
      "SSIM: avg=0.9861, min=0.8317, max=0.9897\n",
      "PSNR: avg=39.36 dB, min=31.47 dB, max=40.59 dB\n",
      "\n",
      "---- Evaluating png_images_800M_1333M ----\n",
      "SSIM: avg=0.9925, min=0.9829, max=0.9955\n",
      "PSNR: avg=41.44 dB, min=35.36 dB, max=42.73 dB\n",
      "\n",
      "---- Evaluating png_images_850M_1416M ----\n",
      "SSIM: avg=0.9698, min=0.9399, max=0.9783\n",
      "PSNR: avg=36.40 dB, min=31.82 dB, max=37.46 dB\n",
      "\n",
      "---- Evaluating png_images_900M_1500M ----\n",
      "SSIM: avg=0.9388, min=0.8828, max=0.9545\n",
      "PSNR: avg=35.19 dB, min=30.32 dB, max=36.40 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FREQ_DIRS = [\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_650M_1083M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_700M_1167M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_800M_1333M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_850M_1416M\",\n",
    "    r\"C:\\Preet\\clean_validation dataset\\png_images_900M_1500M\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE LOOP\n",
    "# =========================\n",
    "for freq_dir in FREQ_DIRS:\n",
    "    print(f\"\\n---- Evaluating {os.path.basename(freq_dir)} ----\")\n",
    "    pred_dir = os.path.join(freq_dir, \"predictions_fk_transunet\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "    low_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_l.png\")])\n",
    "    high_files = numerical_sort([f for f in os.listdir(freq_dir) if f.endswith(\"_h.png\")])\n",
    "\n",
    "    psnr_values, ssim_values = [], []\n",
    "\n",
    "    for i in range(len(low_files)):\n",
    "        low_path = os.path.join(freq_dir, low_files[i])\n",
    "        high_path = os.path.join(freq_dir, high_files[i])\n",
    "\n",
    "        # Load LR\n",
    "        lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = model(lr_tensor).cpu().squeeze().numpy()\n",
    "\n",
    "        # Scale to 0-255\n",
    "        pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "\n",
    "        # Save prediction\n",
    "        pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "        Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "        # Load GT\n",
    "        gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        # Metrics\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "    else:\n",
    "        print(\"[Error] No valid pairs processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88900aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 650 → pred → compare with 1083 ----\n",
      "SSIM: avg=0.9466, min=0.9251, max=0.9574\n",
      "PSNR: avg=27.01 dB, min=26.07 dB, max=27.40 dB\n",
      "\n",
      "---- Stage 2: pred_1083 → pred → compare with 1800 ----\n",
      "SSIM: avg=0.9508, min=0.9226, max=0.9677\n",
      "PSNR: avg=28.89 dB, min=26.36 dB, max=29.64 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\clean_validation dataset\\png_images_650M_1083M_1800M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    \"\"\"Runs inference with proper scaling and output normalization\"\"\"\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    # scale back to [0,255]\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    pred = (pred * 255.0).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"650_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1083_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1800_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_fk_transunet\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_fk_transunet\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32) / 255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 650 → pred → compare with 1083 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1083 → pred → compare with 1800 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd74421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Stage 1: 900 → pred → compare with 1500 ----\n",
      "SSIM: avg=0.7426, min=0.5871, max=0.8675\n",
      "PSNR: avg=17.44 dB, min=15.73 dB, max=18.12 dB\n",
      "\n",
      "---- Stage 2: pred_1500 → pred → compare with 2500 ----\n",
      "SSIM: avg=0.7630, min=0.6372, max=0.8696\n",
      "PSNR: avg=16.75 dB, min=15.39 dB, max=17.40 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_DIR = r\"C:\\Preet\\clean_validation dataset\\png_images_900M_1500M_2500M\"\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0: return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "def run_model(img, model):\n",
    "    \"\"\"Runs inference with proper scaling and output normalization\"\"\"\n",
    "    tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    pred = np.squeeze(pred)\n",
    "    # scale back to [0,255]\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    pred = (pred * 255.0).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE 2-STAGE\n",
    "# =========================\n",
    "files_650 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"900_bscan.png\")])\n",
    "files_1083 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"1500_bscan.png\")])\n",
    "files_1800 = numerical_sort([f for f in os.listdir(DATASET_DIR) if f.endswith(\"2500_bscan.png\")])\n",
    "\n",
    "psnr_stage1, ssim_stage1 = [], []\n",
    "psnr_stage2, ssim_stage2 = [], []\n",
    "\n",
    "pred_dir1 = os.path.join(DATASET_DIR, \"predictions_stage1_fk_transunet\")\n",
    "pred_dir2 = os.path.join(DATASET_DIR, \"predictions_stage2_fk_transunet\")\n",
    "os.makedirs(pred_dir1, exist_ok=True)\n",
    "os.makedirs(pred_dir2, exist_ok=True)\n",
    "\n",
    "for i in range(len(files_650)):\n",
    "    # ---- Stage 1: 650 -> pred -> compare with 1083 ----\n",
    "    lr_img = np.array(Image.open(os.path.join(DATASET_DIR, files_650[i])).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    gt_1083 = cv2.imread(os.path.join(DATASET_DIR, files_1083[i]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    pred_1083 = run_model(lr_img, model)\n",
    "    Image.fromarray(pred_1083).save(os.path.join(pred_dir1, f\"pred1_{i+1}.png\"))\n",
    "\n",
    "    if pred_1083.shape != gt_1083.shape:\n",
    "        pred_1083 = cv2.resize(pred_1083, (gt_1083.shape[1], gt_1083.shape[0]))\n",
    "\n",
    "    psnr_stage1.append(calculate_psnr(pred_1083, gt_1083))\n",
    "    ssim_stage1.append(calculate_ssim(pred_1083, gt_1083))\n",
    "\n",
    "    # ---- Stage 2: pred_1083 -> pred -> compare with 1800 ----\n",
    "    gt_1800 = cv2.imread(os.path.join(DATASET_DIR, files_1800[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    pred_1083_resized = cv2.resize(pred_1083, IMAGE_SIZE).astype(np.float32) / 255.0\n",
    "    pred_1800 = run_model(pred_1083_resized, model)\n",
    "    Image.fromarray(pred_1800).save(os.path.join(pred_dir2, f\"pred2_{i+1}.png\"))\n",
    "\n",
    "    if pred_1800.shape != gt_1800.shape:\n",
    "        pred_1800 = cv2.resize(pred_1800, (gt_1800.shape[1], gt_1800.shape[0]))\n",
    "\n",
    "    psnr_stage2.append(calculate_psnr(pred_1800, gt_1800))\n",
    "    ssim_stage2.append(calculate_ssim(pred_1800, gt_1800))\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n---- Stage 1: 900 → pred → compare with 1500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage1):.4f}, min={np.min(ssim_stage1):.4f}, max={np.max(ssim_stage1):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage1):.2f} dB, min={np.min(psnr_stage1):.2f} dB, max={np.max(psnr_stage1):.2f} dB\")\n",
    "\n",
    "print(\"\\n---- Stage 2: pred_1500 → pred → compare with 2500 ----\")\n",
    "print(f\"SSIM: avg={np.mean(ssim_stage2):.4f}, min={np.min(ssim_stage2):.4f}, max={np.max(ssim_stage2):.4f}\")\n",
    "print(f\"PSNR: avg={np.mean(psnr_stage2):.2f} dB, min={np.min(psnr_stage2):.2f} dB, max={np.max(psnr_stage2):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c94847",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278241a",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da9cf23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Model loaded: C:\\Preet\\clean_paired_bscans\\fk_transunet++.pth\n",
      "✅ Saved prediction for cropped_patch_256x256: C:\\Preet\\Real GPR data\\FK-transunet\\single_image_preds\\cropped_patch_256x256_pred_256x256.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "# ==========================\n",
    "# CONFIG (paths edited)\n",
    "# ==========================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH   = r\"C:\\Preet\\clean_paired_bscans\\fk_transunet++.pth\"\n",
    "IMAGE_DIR    = r\"C:\\Preet\\Real GPR data\"  # where the two PNGs live\n",
    "INPUT_IMAGES = [\n",
    "    os.path.join(IMAGE_DIR, \"cropped_patch_256x256.png\"),\n",
    "]\n",
    "OUTPUT_DIR   = r\"C:\\Preet\\Real GPR data\\FK-transunet\\single_image_preds\"\n",
    "TARGET_SIZE  = (256, 256)  # H, W used by the model\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# MODEL BLOCKS (same as main code)\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size*patch_size*in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size*patch_size*in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        patches = self.flatten(x).transpose(1,2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1,2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H,W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Conv -> PixelShuffle(2x) -> Conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        # Decoder (CUP + skips)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)                      # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))          # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))          # [B,256,H/4,W/4]\n",
    "        b  = self.bottleneck(self.pool(e3))    # [B,256,H/8,W/8]\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# ==========================\n",
    "# HELPERS\n",
    "# ==========================\n",
    "def preprocess(img_path, target_hw=(256, 256)):\n",
    "    \"\"\"Load grayscale, ensure 256x256, normalize to [0,1], -> (1,1,H,W) tensor.\"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Cannot read image: {img_path}\")\n",
    "    if img.shape != target_hw:\n",
    "        img = cv2.resize(img, (target_hw[1], target_hw[0]), interpolation=cv2.INTER_CUBIC)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    tensor = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    return tensor\n",
    "\n",
    "def save_pred(pred_tensor, save_path):\n",
    "    pred = pred_tensor.squeeze().cpu().numpy()\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    pred = (pred * 255).astype(np.uint8)\n",
    "    cv2.imwrite(save_path, pred)\n",
    "\n",
    "# ==========================\n",
    "# MAIN\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    model = TransUNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    print(\"Model loaded:\", MODEL_PATH)\n",
    "\n",
    "    for img_path in INPUT_IMAGES:\n",
    "        name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        out_path = os.path.join(OUTPUT_DIR, f\"{name}_pred_256x256.png\")\n",
    "\n",
    "        x = preprocess(img_path, TARGET_SIZE).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y = model(x)\n",
    "\n",
    "        save_pred(y, out_path)\n",
    "        print(f\"✅ Saved prediction for {name}: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b191645",
   "metadata": {},
   "source": [
    "test for 400-670Mhz on the trained model of 750-1250Mhz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978f9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Evaluating C:\\Preet\\400_670_Dataset ----\n",
      "SSIM: avg=0.7356, min=0.6056, max=0.8002\n",
      "PSNR: avg=27.81 dB, min=27.30 dB, max=29.10 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "TEST_DIR = r\"C:\\Preet\\400_670_Dataset\"\n",
    "MODEL_PATH = r\"C:\\Preet\\clean_paired_bscans\\fk_transunet++.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# =========================\n",
    "# MODEL DEFINITION\n",
    "# =========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        return self.fold(patches)\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x): return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch * 2)\n",
    "        self.enc3 = ConvBlock(base_ch * 2, base_ch * 4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch * 4)\n",
    "        self.up3 = UpCUP(base_ch * 4, base_ch * 2)\n",
    "        self.dec3 = ConvBlock(base_ch * 2 + base_ch * 4, base_ch * 2)\n",
    "        self.up2 = UpCUP(base_ch * 2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch * 2, base_ch)\n",
    "        self.up1 = UpCUP(base_ch, base_ch // 2)\n",
    "        self.dec1 = ConvBlock(base_ch // 2 + base_ch, base_ch // 2)\n",
    "        self.out_conv = nn.Conv2d(base_ch // 2, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "def numerical_sort(files):\n",
    "    return sorted(files, key=lambda f: int(re.search(r'\\d+', f).group()))\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# INFERENCE ON TEST FOLDER\n",
    "# =========================\n",
    "print(f\"\\n---- Evaluating {TEST_DIR} ----\")\n",
    "pred_dir = os.path.join(TEST_DIR, \"predictions_fk_transunet\")\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "low_files = numerical_sort([f for f in os.listdir(TEST_DIR) if f.endswith(\"_l.png\")])\n",
    "high_files = numerical_sort([f for f in os.listdir(TEST_DIR) if f.endswith(\"_h.png\")])\n",
    "\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "for i in range(len(low_files)):\n",
    "    low_path = os.path.join(TEST_DIR, low_files[i])\n",
    "    high_path = os.path.join(TEST_DIR, high_files[i])\n",
    "\n",
    "    # Load LR\n",
    "    lr_img = np.array(Image.open(low_path).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    lr_tensor = torch.tensor(lr_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred = model(lr_tensor).cpu().squeeze().numpy()\n",
    "\n",
    "    # Scale to 0–255\n",
    "    pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    pred_path = os.path.join(pred_dir, f\"pred_{i+1}.png\")\n",
    "    Image.fromarray(pred_img).save(pred_path)\n",
    "\n",
    "    # Load GT\n",
    "    gt_img = cv2.imread(high_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if pred_img.shape != gt_img.shape:\n",
    "        pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "    # Metrics\n",
    "    psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "    ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "if psnr_values and ssim_values:\n",
    "    print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "    print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")\n",
    "else:\n",
    "    print(\"[Error] No valid *_l.png and *_h.png pairs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e6beb7",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa99ecc",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387beef8",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10c5a3",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33fa0a0",
   "metadata": {},
   "source": [
    "training on 400-670 Mhz dataset size -> 3404 paired images -> approx 3400 paired images -> 1700 images LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bd4046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Train: 1361, Test: 341\n",
      "Epoch [1/75] Train 0.022333\n",
      "  ✅ Saved Best Model at Epoch 1\n",
      "Epoch [2/75] Train 0.000778\n",
      "  ✅ Saved Best Model at Epoch 2\n",
      "Epoch [3/75] Train 0.000553\n",
      "  ✅ Saved Best Model at Epoch 3\n",
      "Epoch [4/75] Train 0.000510\n",
      "  ✅ Saved Best Model at Epoch 4\n",
      "Epoch [5/75] Train 0.000378\n",
      "  ✅ Saved Best Model at Epoch 5\n",
      "Epoch [6/75] Train 0.000239\n",
      "  ✅ Saved Best Model at Epoch 6\n",
      "Epoch [7/75] Train 0.000207\n",
      "  ✅ Saved Best Model at Epoch 7\n",
      "Epoch [8/75] Train 0.000165\n",
      "  ✅ Saved Best Model at Epoch 8\n",
      "Epoch [9/75] Train 0.000154\n",
      "  ✅ Saved Best Model at Epoch 9\n",
      "Epoch [10/75] Train 0.000149\n",
      "  ✅ Saved Best Model at Epoch 10\n",
      "Epoch [11/75] Train 0.000135\n",
      "  ✅ Saved Best Model at Epoch 11\n",
      "Epoch [12/75] Train 0.000125\n",
      "  ✅ Saved Best Model at Epoch 12\n",
      "Epoch [13/75] Train 0.000119\n",
      "  ✅ Saved Best Model at Epoch 13\n",
      "Epoch [14/75] Train 0.000124\n",
      "Epoch [15/75] Train 0.000114\n",
      "  ✅ Saved Best Model at Epoch 15\n",
      "Epoch [16/75] Train 0.000174\n",
      "Epoch [17/75] Train 0.000112\n",
      "  ✅ Saved Best Model at Epoch 17\n",
      "Epoch [18/75] Train 0.000106\n",
      "  ✅ Saved Best Model at Epoch 18\n",
      "Epoch [19/75] Train 0.000106\n",
      "Epoch [20/75] Train 0.000101\n",
      "  ✅ Saved Best Model at Epoch 20\n",
      "Epoch [21/75] Train 0.000123\n",
      "Epoch [22/75] Train 0.000101\n",
      "  ✅ Saved Best Model at Epoch 22\n",
      "Epoch [23/75] Train 0.000103\n",
      "Epoch [24/75] Train 0.000095\n",
      "  ✅ Saved Best Model at Epoch 24\n",
      "Epoch [25/75] Train 0.000098\n",
      "Epoch [26/75] Train 0.000105\n",
      "Epoch [27/75] Train 0.000094\n",
      "  ✅ Saved Best Model at Epoch 27\n",
      "Epoch [28/75] Train 0.000095\n",
      "Epoch [29/75] Train 0.000093\n",
      "  ✅ Saved Best Model at Epoch 29\n",
      "Epoch [30/75] Train 0.000095\n",
      "Epoch [31/75] Train 0.000093\n",
      "  ✅ Saved Best Model at Epoch 31\n",
      "Epoch [32/75] Train 0.000096\n",
      "Epoch [33/75] Train 0.000087\n",
      "  ✅ Saved Best Model at Epoch 33\n",
      "Epoch [34/75] Train 0.000090\n",
      "Epoch [35/75] Train 0.000087\n",
      "Epoch [36/75] Train 0.000088\n",
      "Epoch [37/75] Train 0.000089\n",
      "Epoch [38/75] Train 0.000079\n",
      "  ✅ Saved Best Model at Epoch 38\n",
      "Epoch [39/75] Train 0.000089\n",
      "Epoch [40/75] Train 0.000143\n",
      "Epoch [41/75] Train 0.000092\n",
      "Epoch [42/75] Train 0.000084\n",
      "Epoch [43/75] Train 0.000083\n",
      "Epoch [44/75] Train 0.000081\n",
      "Epoch [45/75] Train 0.000082\n",
      "Epoch [46/75] Train 0.000075\n",
      "  ✅ Saved Best Model at Epoch 46\n",
      "Epoch [47/75] Train 0.000084\n",
      "Epoch [48/75] Train 0.000075\n",
      "  ✅ Saved Best Model at Epoch 48\n",
      "Epoch [49/75] Train 0.000071\n",
      "  ✅ Saved Best Model at Epoch 49\n",
      "Epoch [50/75] Train 0.000070\n",
      "  ✅ Saved Best Model at Epoch 50\n",
      "Epoch [51/75] Train 0.000072\n",
      "Epoch [52/75] Train 0.000067\n",
      "  ✅ Saved Best Model at Epoch 52\n",
      "Epoch [53/75] Train 0.000069\n",
      "Epoch [54/75] Train 0.000065\n",
      "  ✅ Saved Best Model at Epoch 54\n",
      "Epoch [55/75] Train 0.000066\n",
      "Epoch [56/75] Train 0.000063\n",
      "  ✅ Saved Best Model at Epoch 56\n",
      "Epoch [57/75] Train 0.000065\n",
      "Epoch [58/75] Train 0.000064\n",
      "Epoch [59/75] Train 0.000064\n",
      "Epoch [60/75] Train 0.000060\n",
      "  ✅ Saved Best Model at Epoch 60\n",
      "Epoch [61/75] Train 0.000062\n",
      "Epoch [62/75] Train 0.000062\n",
      "Epoch [63/75] Train 0.000059\n",
      "  ✅ Saved Best Model at Epoch 63\n",
      "Epoch [64/75] Train 0.000059\n",
      "Epoch [65/75] Train 0.000058\n",
      "  ✅ Saved Best Model at Epoch 65\n",
      "Epoch [66/75] Train 0.000058\n",
      "Epoch [67/75] Train 0.000061\n",
      "Epoch [68/75] Train 0.000057\n",
      "  ✅ Saved Best Model at Epoch 68\n",
      "Epoch [69/75] Train 0.000055\n",
      "  ✅ Saved Best Model at Epoch 69\n",
      "Epoch [70/75] Train 0.000055\n",
      "  ✅ Saved Best Model at Epoch 70\n",
      "Epoch [71/75] Train 0.000053\n",
      "  ✅ Saved Best Model at Epoch 71\n",
      "Epoch [72/75] Train 0.000052\n",
      "  ✅ Saved Best Model at Epoch 72\n",
      "Epoch [73/75] Train 0.000053\n",
      "Epoch [74/75] Train 0.000054\n",
      "Epoch [75/75] Train 0.000054\n",
      "\n",
      "Running inference on test set...\n",
      "✅ Predictions saved with matching labels in C:\\Preet\\png_images_400M_670M\\predictions_fk_transunet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "DATASET_DIR = r\"C:\\Preet\\png_images_400M_670M\"  # *_l.png and *_h.png paired images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 75\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "MODEL_PATH = os.path.join(DATASET_DIR, \"fk_transunet++.pth\")\n",
    "RESULTS_DIR = os.path.join(DATASET_DIR, \"predictions_fk_transunet\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# f–k loss weights / options (tune if needed)\n",
    "FK_WEIGHT = 0.5           # contribution of f–k term to total loss\n",
    "FK_USE_LOG = True         # use log magnitude for stability\n",
    "FK_USE_HANN = True        # apply 2D Hann window before FFT to reduce leakage\n",
    "FK_KY = (0.05, 0.85)      # rectangular band in normalized ky (temporal freq)\n",
    "FK_KX = (0.00, 0.90)      # rectangular band in normalized kx (spatial wavenumber)\n",
    "\n",
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class GPRDataset(Dataset):\n",
    "    def __init__(self, x_paths, y_paths):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.array(Image.open(self.x_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        y = np.array(Image.open(self.y_paths[idx]).resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "        x = torch.tensor(x).unsqueeze(0)  # (1,H,W)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        return x, y\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    low_paths, high_paths = [], []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\"_l.png\"):\n",
    "            low_path = os.path.join(dataset_dir, file)\n",
    "            high_path = os.path.join(dataset_dir, file.replace(\"_l.png\", \"_h.png\"))\n",
    "            if os.path.exists(high_path):\n",
    "                low_paths.append(low_path)\n",
    "                high_paths.append(high_path)\n",
    "    return low_paths, high_paths\n",
    "\n",
    "# ==========================\n",
    "# DATA SPLIT (80:20 Train/Test)\n",
    "# ==========================\n",
    "all_x, all_y = load_data(DATASET_DIR)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    all_x, all_y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_x)}, Test: {len(test_x)}\")\n",
    "\n",
    "train_loader = DataLoader(GPRDataset(train_x, train_y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(GPRDataset(test_x, test_y), batch_size=1, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None  # initialized at runtime\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)  # (B, N, patch_dim)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "# ---- NEW: CUP upsampler block ----\n",
    "class UpCUP(nn.Module):\n",
    "    \"\"\"Detail-preserving upsampler: conv -> pixelshuffle(2x) -> conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),                         # (H,W) -> (2H,2W)\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        # ================= Encoder =================\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)            # 1 -> 64\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)        # 64 -> 128\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)      # 128 -> 256\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ================= Bottleneck =================\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "\n",
    "        # ================= Decoder (CUP) =================\n",
    "        # stage with e3 skip\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)                         # 256 -> 128 @ H/4\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)         # [128 + 256] -> 128\n",
    "        # stage with e2 skip\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)                           # 128 -> 64  @ H/2\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)             # [64 + 128] -> 64\n",
    "        # NEW: stage with e1 (high-res) skip\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)                          # 64 -> 32  @ H\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)         # [32 + 64] -> 32\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)    # 32 -> 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------- Encoder ----------------\n",
    "        e1 = self.enc1(x)                       # [B,64,H,W]\n",
    "        e2 = self.enc2(self.pool(e1))           # [B,128,H/2,W/2]\n",
    "        e3 = self.enc3(self.pool(e2))           # [B,256,H/4,W/4]\n",
    "\n",
    "        # ---------------- Bottleneck ----------------\n",
    "        b = self.bottleneck(self.pool(e3))      # [B,256,H/8,W/8]\n",
    "\n",
    "        # ---------------- Decoder (CUP + all skips) ----------------\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))   # -> [B,128,H/4,W/4]\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))   # -> [B,64, H/2,W/2]\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))   # -> [B,32, H,  W]\n",
    "        out = self.out_conv(d1)                                # -> [B,1,  H,  W]\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# f–k LOSS (frequency–wavenumber consistency)\n",
    "# ==========================\n",
    "def make_rect_fk_mask(H, W, device, ky=(0.05, 0.85), kx=(0.00, 0.90)):\n",
    "    \"\"\"\n",
    "    Simple rectangular bandpass in normalized frequency space.\n",
    "    ky, kx are fractions of Nyquist in [0,1].\n",
    "    \"\"\"\n",
    "    yy = torch.linspace(-1, 1, H, device=device).abs().view(H, 1).expand(H, W)\n",
    "    xx = torch.linspace(-1, 1, W, device=device).abs().view(1, W).expand(H, W)\n",
    "    m = ((yy >= ky[0]) & (yy <= ky[1]) & (xx >= kx[0]) & (xx <= kx[1])).float()\n",
    "    return m.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "def fk_loss(pred, target, bandpass=None, p=1, use_log=True, window=True):\n",
    "    \"\"\"\n",
    "    pred/target: (B,1,H,W) in [0,1]\n",
    "    Compare spectra in f–k domain (2D FFT). L1 by default.\n",
    "    \"\"\"\n",
    "    assert pred.shape == target.shape\n",
    "    if window:\n",
    "        H, W = pred.shape[-2:]\n",
    "        wy = torch.hann_window(H, device=pred.device).view(1,1,H,1)\n",
    "        wx = torch.hann_window(W, device=pred.device).view(1,1,1,W)\n",
    "        win = wy * wx\n",
    "        pred = pred * win\n",
    "        target = target * win\n",
    "\n",
    "    P = torch.fft.fft2(pred, norm='ortho')\n",
    "    T = torch.fft.fft2(target, norm='ortho')\n",
    "    Pm = torch.abs(P)\n",
    "    Tm = torch.abs(T)\n",
    "    if use_log:\n",
    "        Pm = torch.log1p(Pm)\n",
    "        Tm = torch.log1p(Tm)\n",
    "    diff = (Pm - Tm).abs() if p == 1 else (Pm - Tm).pow(2)\n",
    "    if bandpass is not None:\n",
    "        diff = diff * bandpass.to(diff.dtype)\n",
    "    return diff.mean()\n",
    "\n",
    "# ==========================\n",
    "# TRAINING\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "huber = nn.HuberLoss(delta=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_train_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "\n",
    "        # Combine Huber + f–k loss\n",
    "        fk_mask = make_rect_fk_mask(y.shape[2], y.shape[3], y.device, ky=FK_KY, kx=FK_KX)\n",
    "        loss_huber = huber(preds, y)\n",
    "        loss_fk = fk_loss(preds.clamp(0,1), y, bandpass=fk_mask,\n",
    "                          p=1, use_log=FK_USE_LOG, window=FK_USE_HANN)\n",
    "        loss = loss_huber + FK_WEIGHT * loss_fk\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Train {train_loss:.6f}\")\n",
    "\n",
    "    # Save best model based on training loss\n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  ✅ Saved Best Model at Epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE (Save with matching test image numbers)\n",
    "# ==========================\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "for x_path, y_path in zip(test_x, test_y):\n",
    "    # Extract numeric ID from filename (e.g., '310' from '310bscan_l.png')\n",
    "    base_name = os.path.basename(x_path)\n",
    "    num_label = ''.join([c for c in base_name if c.isdigit()])\n",
    "    if not num_label:\n",
    "        num_label = os.path.splitext(base_name)[0]  # fallback to full name\n",
    "\n",
    "    x = Image.open(x_path).convert('L')\n",
    "    x = np.array(x.resize(IMAGE_SIZE), dtype=np.float32) / 255.0\n",
    "    x = torch.tensor(x).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).cpu().squeeze().numpy()\n",
    "\n",
    "    pred_img = (np.clip(pred, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    save_path = os.path.join(RESULTS_DIR, f\"pred_{num_label}.png\")\n",
    "    Image.fromarray(pred_img).save(save_path)\n",
    "\n",
    "print(f\"✅ Predictions saved with matching labels in {RESULTS_DIR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb8fbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Test Set Evaluation ----\n",
      "SSIM: avg=0.9583, min=0.8245, max=0.9983\n",
      "PSNR: avg=42.84 dB, min=29.97 dB, max=54.73 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "import shutil\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "RESULTS_DIR = r\"C:\\Preet\\png_images_400M_670M\\predictions_fk_transunet\"  # predictions from TransUNet\n",
    "GT_TEST_DIR = r\"C:\\Preet\\png_images_400M_670M\\ground_truth_test_fk_transunet\"\n",
    "\n",
    "# Copy ground truth test images to a dedicated folder\n",
    "os.makedirs(GT_TEST_DIR, exist_ok=True)\n",
    "for f in test_y:  # test_y comes from your train/val/test split\n",
    "    shutil.copy(f, GT_TEST_DIR)\n",
    "\n",
    "# ========================== FUNCTIONS ==========================\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255)\n",
    "\n",
    "# ========================== MAIN EVALUATION ==========================\n",
    "psnr_values, ssim_values = [], []\n",
    "\n",
    "pred_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "gt_files   = sorted([f for f in os.listdir(GT_TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "num_pairs = min(len(pred_files), len(gt_files))\n",
    "if num_pairs == 0:\n",
    "    print(\"[Error] No matching image files found in both directories.\")\n",
    "else:\n",
    "    if len(pred_files) != len(gt_files):\n",
    "        print(f\"[Warning] Different number of images: Predictions={len(pred_files)}, Ground Truth={len(gt_files)}\")\n",
    "        print(f\"Evaluating only first {num_pairs} matched pairs.\")\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        pred_path = os.path.join(RESULTS_DIR, pred_files[i])\n",
    "        gt_path   = os.path.join(GT_TEST_DIR, gt_files[i])\n",
    "\n",
    "        pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_img   = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if pred_img is None or gt_img is None:\n",
    "            print(f\"[Error] Could not load: {pred_files[i]} or {gt_files[i]}\")\n",
    "            continue\n",
    "\n",
    "        if pred_img.shape != gt_img.shape:\n",
    "            pred_img = cv2.resize(pred_img, (gt_img.shape[1], gt_img.shape[0]))\n",
    "\n",
    "        psnr_values.append(calculate_psnr(pred_img, gt_img))\n",
    "        ssim_values.append(calculate_ssim(pred_img, gt_img))\n",
    "\n",
    "    if psnr_values and ssim_values:\n",
    "        print(f\"\\n---- Test Set Evaluation ----\")\n",
    "        print(f\"SSIM: avg={np.mean(ssim_values):.4f}, min={np.min(ssim_values):.4f}, max={np.max(ssim_values):.4f}\")\n",
    "        print(f\"PSNR: avg={np.mean(psnr_values):.2f} dB, min={np.min(psnr_values):.2f} dB, max={np.max(psnr_values):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4b2e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from C:\\Preet\\png_images_400M_670M\\fk_transunet++.pth\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_1.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_2.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_3.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_4.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_5.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_6.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_7.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_8.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_9.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_10.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_11.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_12.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_13.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_14.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_15.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_16.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_17.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_18.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_19.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_20.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_21.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_22.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_23.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_24.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_25.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_26.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_27.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_28.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_29.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_30.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_31.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_32.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_33.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_34.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_35.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_36.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_37.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_38.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_39.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_40.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_41.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_42.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_43.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_44.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_45.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_46.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_47.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_48.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_49.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_50.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_51.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_52.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_53.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_54.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_55.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_56.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_57.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_58.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_59.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_60.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_61.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_62.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_63.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_64.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_65.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_66.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_67.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_68.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_69.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_70.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_71.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_72.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_73.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_74.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_75.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_76.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_77.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_78.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_79.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_80.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_81.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_82.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_83.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_84.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_85.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_86.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_87.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_88.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_89.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_90.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_91.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_92.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_93.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_94.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_95.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_96.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_97.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_98.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_99.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_100.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_101.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_102.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_103.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_104.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_105.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_106.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_107.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_108.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_109.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_110.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_111.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_112.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_113.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_114.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_115.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_116.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_117.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_118.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_119.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_120.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_121.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_122.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_123.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_124.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_125.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_126.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_127.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_128.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_129.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_130.png\n",
      "Saved: C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\\pred_131.png\n",
      "\n",
      "✅ All predictions saved to C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\n"
     ]
    }
   ],
   "source": [
    "# test real gpr data from mendaley's\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "TEST_DIR   = r\"C:\\Preet\\Real GPR data\\mendaley\\Utilities_resized\"  # folder with real GPR images\n",
    "IMAGE_SIZE = (256, 256)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_PATH = r\"C:\\Preet\\png_images_400M_670M\\fk_transunet++.pth\"   # trained weights\n",
    "RESULTS_DIR = r\"C:\\Preet\\Real GPR data\\mendaley\\Utilities_predictions_fk\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# MODEL BLOCKS (copied from training script)\n",
    "# ==========================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.drop(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.drop(ff_out))\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, patch_size=16, num_layers=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = in_ch\n",
    "        self.flatten = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.project = nn.Linear(patch_size * patch_size * in_ch, in_ch)\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim=in_ch, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.reconstruct = nn.Linear(in_ch, patch_size * patch_size * in_ch)\n",
    "        self.fold = None\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = self.flatten(x).transpose(1, 2)\n",
    "        patches = self.project(patches)\n",
    "        patches = self.transformer(patches)\n",
    "        patches = self.reconstruct(patches).transpose(1, 2)\n",
    "        if self.fold is None:\n",
    "            self.fold = nn.Fold(output_size=(H, W),\n",
    "                                kernel_size=self.patch_size,\n",
    "                                stride=self.patch_size)\n",
    "        x_reconstructed = self.fold(patches)\n",
    "        return x_reconstructed\n",
    "\n",
    "class UpCUP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch * 4, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = TransformerBottleneck(base_ch*4, patch_size=16, num_layers=2, num_heads=4)\n",
    "        self.up3  = UpCUP(base_ch*4, base_ch*2)\n",
    "        self.dec3 = ConvBlock(base_ch*2 + base_ch*4, base_ch*2)\n",
    "        self.up2  = UpCUP(base_ch*2, base_ch)\n",
    "        self.dec2 = ConvBlock(base_ch + base_ch*2, base_ch)\n",
    "        self.up1  = UpCUP(base_ch, base_ch//2)\n",
    "        self.dec1 = ConvBlock(base_ch//2 + base_ch, base_ch//2)\n",
    "        self.out_conv = nn.Conv2d(base_ch//2, out_ch, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        b  = self.bottleneck(self.pool(e3))\n",
    "        d3 = self.dec3(torch.cat([self.up3(b),  e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LOAD MODEL\n",
    "# ==========================\n",
    "model = TransUNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "print(f\"Loaded model from {MODEL_PATH}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# INFERENCE ON REAL IMAGES\n",
    "# ==========================\n",
    "files = [f for f in os.listdir(TEST_DIR) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))]\n",
    "\n",
    "for i, fname in enumerate(sorted(files)):\n",
    "    path = os.path.join(TEST_DIR, fname)\n",
    "    img = Image.open(path).convert(\"L\").resize(IMAGE_SIZE)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    inp = torch.tensor(arr).unsqueeze(0).unsqueeze(0).to(DEVICE)  # (1,1,H,W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(inp).cpu().squeeze().numpy()\n",
    "\n",
    "    pred_img = (np.clip(pred, 0, 1) * 255).astype(np.uint8)\n",
    "    out_path = os.path.join(RESULTS_DIR, f\"pred_{i+1}.png\")\n",
    "    Image.fromarray(pred_img).save(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "print(f\"\\n✅ All predictions saved to {RESULTS_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
